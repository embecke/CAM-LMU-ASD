{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01d18f-013b-490a-9c0a-22506f3ec0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "l3s4a: per day and all day correlation and other associated analyses of subjective and objective measures (TET and wristband derived respectively)\n",
    "\n",
    "PART - 1\n",
    "\n",
    "Combination of l3s2 and l3s3 for subjective and objective measures (TET and wristband per min aggr data derived respectively)\n",
    "\n",
    "1. Load the script with subjective dimensions like in l3s2 until the big dictionary step.\n",
    "2. Similarly, load per min agg objective values like in l3s3 until the big dictionary step.\n",
    "3. Combine the two dictionaries by matching the key values (the days)\n",
    "4. Correlate all of them with one another and output into corellogram for per day and all day and both with fdr correction - they should all be matched by the same 15 minute bins\n",
    "\n",
    "\n",
    "PART - 2\n",
    "\n",
    "Any other useful analyses and visualisations\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de2d23-55fb-4e3a-b003-cde69a825654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and major functions\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import kendalltau\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "\n",
    "from l2script_functions import giv_x_y_vals, give_binned_vals, give_binned_vals_category\n",
    "\n",
    "import warnings\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "define binning function for objective measures\n",
    "\"\"\"\n",
    "def give_binned_vals_obj_meas(df, obj_meas, hour, half_hour, quarter_hour, timezone, category_yn = False):\n",
    "    \n",
    "    bin_dict = {}\n",
    "    bin_dict_scl = {}\n",
    "    bin_dict_scr = {}\n",
    "\n",
    "    #only for objective measures, include the extra hour of data (if in cet)\n",
    "    if category_yn:\n",
    "        bin_arr = np.arange(0,25,6)\n",
    "    elif hour: #hour seperation\n",
    "        bin_arr = np.arange(0,25)\n",
    "    elif half_hour: #half hour seperation\n",
    "        bin_arr = np.arange(0,24.5, 0.5)\n",
    "    elif quarter_hour: #quarter hour seperation\n",
    "        bin_arr = np.arange(0,24.25, 0.25)\n",
    "    # Defining the time zones\n",
    "    utc_zone = pytz.utc\n",
    "    req_zone = pytz.timezone(timezone)\n",
    "    \n",
    "    #if aggr_p_min data, time conversion block (add an extra column to the dataframe with required timezone timestamps)\n",
    "    def from_isoutc_to_req(iso_timestamp):\n",
    "            #Parsing the ISO 8601 timestamp into a datetime object\n",
    "            utc_time = datetime.fromisoformat(iso_timestamp.replace(\"Z\", \"+00:00\"))\n",
    "    \n",
    "            #Converting from utc to required (cet) time\n",
    "            req_time = utc_time.astimezone(req_zone)\n",
    "            #print(req_time, type(req_time))\n",
    "    \n",
    "            return req_time \n",
    "\n",
    "    # Apply the conversion function to the 'utc_timestamps' column and create a new column 'converted_timestamps'\n",
    "    df['converted_timestamps'] = df['timestamp_iso'].apply(from_isoutc_to_req)\n",
    "\n",
    "    first_day = df['converted_timestamps'].iloc[0].day \n",
    "\n",
    "    x_val = df['converted_timestamps'].apply(\n",
    "        lambda x: (24 + int(str(x).split()[1].split(':')[0]) + int(str(x).split()[1].split(':')[1])/60) \n",
    "        if x.day > first_day \n",
    "        else (int(str(x).split()[1].split(':')[0]) + int(str(x).split()[1].split(':')[1])/60)\n",
    "    ).tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    #Optional, not used so far\n",
    "    #range of permissible eda range\n",
    "    min_val = 0.05\n",
    "    max_val = 60\n",
    "    #yes but what about scl specifically??? -> haven't found a source talking about scl permissible ranges so for now set this aside\n",
    "    \"\"\"\n",
    "    if obj_meas == 'eda':\n",
    "        y_val = df['eda_scl_usiemens'].tolist()\n",
    "    elif obj_meas == 'pulse_rate':\n",
    "        y_val = df['pulse_rate_bpm'].tolist()\n",
    "    elif obj_meas == 'prv':\n",
    "        y_val = df['prv_rmssd_ms'].tolist()\n",
    "    elif obj_meas == 'resp_rate':\n",
    "        y_val = df['respiratory_rate_brpm'].tolist()\n",
    "    elif obj_meas == 'temp':\n",
    "        y_val = df['temperature_celsius'].tolist()\n",
    "    elif obj_meas == 'step_count':\n",
    "        y_val = df['step_counts'].tolist()\n",
    "    elif obj_meas == 'acc_std':\n",
    "        y_val = df['accelerometers_std_g'].tolist()\n",
    "    elif obj_meas == 'activity_counts':\n",
    "        y_val = df['activity_counts'].tolist()\n",
    "    elif obj_meas == 'met':\n",
    "        y_val = df['met'].tolist() \n",
    "    elif obj_meas == 'wearing_det':\n",
    "        y_val = df['wearing_detection_percentage'].tolist()\n",
    "    ###Optional: include measures after review\n",
    "    \n",
    "    for i in range(0, len(bin_arr) - 1):\n",
    "            #Create the key for the dictionary\n",
    "            key = str(bin_arr[i]) + '_' + str(bin_arr[i+1])\n",
    "    \n",
    "            #Initialize an empty list for this key\n",
    "            templst = []\n",
    "            bin_dict[key] = templst\n",
    "        \n",
    "            #Iterate over x_val, append to templst if condition is met\n",
    "            for j in range(0, len(x_val)):\n",
    "                if x_val[j] >= bin_arr[i] and x_val[j] < bin_arr[i+1] and x_val[j]<bin_arr[-1]: \n",
    "                    #Appending y_val[j] directly to the list in the dictionary\n",
    "                    bin_dict[key].append(y_val[j])\n",
    "                if x_val[j]>=bin_arr[-1]:\n",
    "                    #print(\"not appending value at time: \", x_val[j])\n",
    "                    continue\n",
    "\n",
    "    #for conversion of lists to numpy arrays\n",
    "    for key in bin_dict:\n",
    "            bin_dict[key] = np.array(bin_dict[key])\n",
    "            #print(bin_dict)\n",
    "\n",
    "    bin_dict_mean = {}\n",
    "    for key in bin_dict:\n",
    "            if np.all(np.isnan(bin_dict[key])):\n",
    "                #print('list only has nan values')\n",
    "                bin_dict_mean[key] = np.nan\n",
    "            elif ~np.all(np.isnan(bin_dict[key])) and len(bin_dict[key])!=0:\n",
    "                #print('list is not empty')\n",
    "                bin_dict_mean[key] = np.nanmean(bin_dict[key]) \n",
    "            else:\n",
    "                print('-5000 has been appended') #this print statement added to debug if there is ever a situation where this would happen (technically it shouldn't)\n",
    "                bin_dict_mean[key] = -5000\n",
    "\n",
    "    return  df['converted_timestamps'], x_val, y_val, bin_dict_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18ee4b-7484-4ace-89c2-545442ceae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some more common global variable definitions\n",
    "\n",
    "folder1 = 'empatica'\n",
    "folder2 = 'saved_figures'\n",
    "\n",
    "folder11 = 'aggr_p_min'\n",
    "folder12 = 'avro_files'\n",
    "folder13 = 'avro2csv'\n",
    "folder14 = 'preprocessed_files_debug'\n",
    "folder141 = 'data_preproc_debug'\n",
    "\n",
    "ger = True #if country of data collection is Germany, true, else (if UK, false) -> because language of TET questions and other input data config changes by country\n",
    "timezone = 'Europe/Berlin' # 'utc' #default timezone; enter required timezone if different\n",
    "mainfolder = input('enter participant folder: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182be21-1641-451d-8e60-8a2df4cbcb8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_TET_x1, dict_TET_y1,  x_new1, y_new1, x1, y1 = giv_x_y_vals(mainfolder, 'q1', ger) #has duplicates (asd_001)\n",
    "dict_TET_x2, dict_TET_y2,  x_new2, y_new2, x2, y2 = giv_x_y_vals(mainfolder, 'q2', ger) #has duplicates and days with missing data (asd_001)\n",
    "dict_TET_x3, dict_TET_y3,  x_new3, y_new3, x3, y3 = giv_x_y_vals(mainfolder, 'q3', ger) #has duplicates (hc_002) #has days with missing data (asd_001)\n",
    "dict_TET_x4, dict_TET_y4,  x_new4, y_new4, x4, y4 = giv_x_y_vals(mainfolder, 'q4', ger) #has days with missing data (asd_001)\n",
    "dict_TET_x5, dict_TET_y5,  x_new5, y_new5, x5, y5 = giv_x_y_vals(mainfolder, 'q5', ger) #has duplicates and days with missing data (asd_001)\n",
    "dict_TET_x6, dict_TET_y6,  x_new6, y_new6, x6, y6 = giv_x_y_vals(mainfolder, 'q6', ger) #has duplicates (hc_002) #has days with missing data (asd_001)\n",
    "dict_TET_x7, dict_TET_y7,  x_new7, y_new7, x7, y7 = giv_x_y_vals(mainfolder, 'q7', ger)\n",
    "dict_TET_x8, dict_TET_y8,  x_new8, y_new8, x8, y8 = giv_x_y_vals(mainfolder, 'q8', ger) #has duplicates and days with missing data (asd_001) -> but for the day that it had duplicate data (15_3_24_n7_16_3_24_d) the data was identical so all good\n",
    "dict_TET_x9, dict_TET_y9,  x_new9, y_new9, x9, y9 = giv_x_y_vals(mainfolder, 'q9', ger) #has duplicates (hc_002) #has days with missing data (asd_001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4218ce-9b8f-4324-85c3-23090fd940a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, obtain the binned dictionaries for each dimension\n",
    "\n",
    "dict_TET_x = [dict_TET_x1, dict_TET_x2, dict_TET_x3, dict_TET_x4, dict_TET_x5, dict_TET_x6, dict_TET_x7, dict_TET_x8, dict_TET_x9]\n",
    "dict_TET_y = [dict_TET_y1, dict_TET_y2, dict_TET_y3, dict_TET_y4, dict_TET_y5, dict_TET_y6, dict_TET_y7, dict_TET_y8, dict_TET_y9]\n",
    "\n",
    "dim_q = {}\n",
    "\n",
    "#pairing up the names of the dimensions for added info\n",
    "dim_names = ['wakefullness', 'boredom', 'sensory_avoidance', 'social avoidance', 'physical tension', 'scenario_anxiety', 'rumination', 'stress', 'pain'] #can add 'personalised_dimension' as and when it becomes applicable. But most of the times, data is not available\n",
    "for i in range(9):\n",
    "    dim_q[f'dim_{i+1}_{dim_names[i]}'] = {}\n",
    "    for key in dict_TET_x[i]:\n",
    "        x_val = dict_TET_x[i][key] * 6\n",
    "        y_val = dict_TET_y[i][key]\n",
    "        dim_q[f'dim_{i+1}_{dim_names[i]}'][key] = give_binned_vals(x_val, y_val, '15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc293431-942a-48a6-87dd-04feb4a1c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for each day, assess each dimension array for normality using shapiro wilk and generate correlograms (pearson and kendall). add an indication for every significant test statistic (p<0.05)\n",
    "\n",
    "#step 1: for every day, every available dimension assessed for normality and reported in a single dictionary or dataframe\n",
    "\n",
    "norm_res = {}\n",
    "for subfolder in os.listdir(mainfolder):\n",
    "    if subfolder.endswith('d'):\n",
    "        for i in range(9):\n",
    "            if subfolder in dim_q[f'dim_{i+1}_{dim_names[i]}'].keys():\n",
    "                #print('yes ', subfolder,  f'dim_{i+1}_{dim_names[i]}')\n",
    "                filt_dict = {key: value for key, value in dim_q[f'dim_{i+1}_{dim_names[i]}'][subfolder].items() if value != -5000}\n",
    "                #print(filt_dict)\n",
    "                if subfolder not in norm_res:\n",
    "                        norm_res[subfolder] = {}\n",
    "                if f'dim_{i+1}_{dim_names[i]}' not in norm_res[subfolder]:\n",
    "                        norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}'] = {}\n",
    "                    \n",
    "                if len(filt_dict) >= 3:\n",
    "                    stat, p_value = shapiro(list(filt_dict.values()))\n",
    "                    #print(stat, p_value)\n",
    "                    \n",
    "                    \n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['length of data'] = len(list(filt_dict.values()))\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['stat'] = stat\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['p_value'] = p_value\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['full data'] = dim_q[f'dim_{i+1}_{dim_names[i]}'][subfolder]\n",
    "                else:\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['length of data'] = len(list(filt_dict.values()))\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['stat'] = None\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['p_value'] = None  # Not enough data for the test\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['full data'] = dim_q[f'dim_{i+1}_{dim_names[i]}'][subfolder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff2e9a-01ec-4486-bfb7-c7e35b9a0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_dict_plot_x = {}\n",
    "eda_dict_plot_y = {}\n",
    "eda_dict_bin = {}\n",
    "pulse_rate_dict_plot_x = {}\n",
    "pulse_rate_dict_plot_y = {}\n",
    "pulse_rate_dict_bin = {}\n",
    "prv_dict_plot_x = {}\n",
    "prv_dict_plot_y = {}\n",
    "prv_dict_bin = {}\n",
    "resp_rate_dict_plot_x = {}\n",
    "resp_rate_dict_plot_y = {}\n",
    "resp_rate_dict_bin = {}\n",
    "temp_dict_plot_x = {}\n",
    "temp_dict_plot_y = {}\n",
    "temp_dict_bin = {}\n",
    "step_dict_plot_x = {}\n",
    "step_dict_plot_y = {}\n",
    "step_dict_bin = {}\n",
    "acc_std_dict_plot_x = {}\n",
    "acc_std_dict_plot_y = {}\n",
    "acc_std_dict_bin = {}\n",
    "activity_dict_plot_x = {}\n",
    "activity_dict_plot_y = {}\n",
    "activity_dict_bin = {}\n",
    "met_dict_plot_x = {}\n",
    "met_dict_plot_y = {}\n",
    "met_dict_bin = {}\n",
    "wearing_det_dict_plot_x = {}\n",
    "wearing_det_dict_plot_y = {}\n",
    "wearing_det_dict_bin = {}\n",
    "\n",
    "\n",
    "\n",
    "#in the lines below, take out \"_converted_timestamp,\" variables after timestamp verification check\n",
    "eda_converted_timestamp = {}\n",
    "pulse_rate_converted_timestamp = {}\n",
    "prv_converted_timestamp = {}\n",
    "resp_rate_converted_timestamp = {}\n",
    "temp_converted_timestamp = {}\n",
    "step_converted_timestamp = {}\n",
    "acc_converted_timestamp = {}\n",
    "activity_converted_timestamp = {}\n",
    "met_converted_timestamp = {}\n",
    "wearing_det_converted_timestamp = {}\n",
    "\n",
    "#storing the dates for which the variables are recorded. Required for time-stitching\n",
    "eda_dates = []\n",
    "pulse_rate_dates = []\n",
    "prv_dates = []\n",
    "resp_rate_dates = []\n",
    "temp_dates = []\n",
    "step_dates = []\n",
    "acc_std_dates = []\n",
    "activity_dates = []\n",
    "met_dates = []\n",
    "wearing_det_dates = []\n",
    "\n",
    "\n",
    "for subfolder in os.listdir(mainfolder):\n",
    "    if subfolder.endswith('d') and os.path.exists(os.path.join(mainfolder, subfolder, folder1, folder11)):\n",
    "        print(subfolder)\n",
    "        for file in os.listdir(os.path.join(mainfolder, subfolder, folder1, folder11)):\n",
    "            if file.endswith('eda.csv'):\n",
    "                eda_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                eda_converted_timestamp[subfolder], eda_dict_plot_x[subfolder], eda_dict_plot_y[subfolder], eda_dict_bin[subfolder] = give_binned_vals_obj_meas(eda_df, 'eda', False, False, True, timezone)\n",
    "                eda_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('pulse-rate.csv'):\n",
    "                pulse_rate_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                pulse_rate_converted_timestamp[subfolder], pulse_rate_dict_plot_x[subfolder], pulse_rate_dict_plot_y[subfolder], pulse_rate_dict_bin[subfolder] = give_binned_vals_obj_meas(pulse_rate_df, 'pulse_rate', False, False, True, timezone)\n",
    "                pulse_rate_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('prv.csv'):\n",
    "                prv_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                prv_converted_timestamp[subfolder], prv_dict_plot_x[subfolder], prv_dict_plot_y[subfolder], prv_dict_bin[subfolder] = give_binned_vals_obj_meas(prv_df, 'prv', False, False, True, timezone)\n",
    "                prv_dates.append(subfolder)\n",
    "                \n",
    "            elif file.endswith('respiratory-rate.csv'):\n",
    "                resp_rate_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                resp_rate_converted_timestamp[subfolder], resp_rate_dict_plot_x[subfolder], resp_rate_dict_plot_y[subfolder], resp_rate_dict_bin[subfolder] = give_binned_vals_obj_meas(resp_rate_df, 'resp_rate', False, False, True, timezone)\n",
    "                resp_rate_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('temperature.csv'):\n",
    "                temp_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                temp_converted_timestamp[subfolder], temp_dict_plot_x[subfolder], temp_dict_plot_y[subfolder], temp_dict_bin[subfolder] = give_binned_vals_obj_meas(temp_df, 'temp', False, False, True, timezone)\n",
    "                temp_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('step-counts.csv'):\n",
    "                step_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                step_converted_timestamp[subfolder], step_dict_plot_x[subfolder], step_dict_plot_y[subfolder], step_dict_bin[subfolder] = give_binned_vals_obj_meas(step_df, 'step_count', False, False, True, timezone)\n",
    "                step_dates.append(subfolder)\n",
    "                \n",
    "            elif file.endswith('accelerometers-std.csv'):\n",
    "                acc_std_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                acc_converted_timestamp[subfolder], acc_std_dict_plot_x[subfolder], acc_std_dict_plot_y[subfolder], acc_std_dict_bin[subfolder] = give_binned_vals_obj_meas(acc_std_df, 'acc_std', False, False, True, timezone)\n",
    "                acc_std_dates.append(subfolder)\n",
    "                \n",
    "            elif file.endswith('activity-counts.csv'):\n",
    "                activity_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                activity_converted_timestamp[subfolder], activity_dict_plot_x[subfolder], activity_dict_plot_y[subfolder], activity_dict_bin[subfolder] = give_binned_vals_obj_meas(activity_df, 'activity_counts', False, False, True, timezone)\n",
    "                activity_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('met.csv'):\n",
    "                met_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                met_converted_timestamp[subfolder], met_dict_plot_x[subfolder], met_dict_plot_y[subfolder], met_dict_bin[subfolder] = give_binned_vals_obj_meas(met_df, 'met', False, False, True, timezone)\n",
    "                met_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('wearing-detection.csv'):\n",
    "                wearing_det_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                wearing_det_converted_timestamp[subfolder], wearing_det_dict_plot_x[subfolder], wearing_det_dict_plot_y[subfolder], wearing_det_dict_bin[subfolder] = give_binned_vals_obj_meas(wearing_det_df, 'wearing_det', False, False, True, timezone)\n",
    "                wearing_det_dates.append(subfolder)                \n",
    "    #(df, obj_meas, hour, half_hour, quarter_hour, timezone):\n",
    "    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ca9b8-d9df-4d1b-a118-2fb5a8f584ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional function for timestitch\n",
    "def process_time_values(non_cont_dates, dates_list, dict_plot_x, dict_plot_y):\n",
    "    \"\"\"\n",
    "    Process time values to move data points with x >= 24 to the next day.\n",
    "    Also creates binned data and calculates bin means for each date.\n",
    "    \n",
    "    Parameters:\n",
    "    non_cont_dates (list): List of dates that are not continuous i.e; dates where the next night is not the very next date but further off. \n",
    "    dates_list (list): List of dates for the specific measure\n",
    "    dict_plot_x (dict): Dictionary with dates as keys and x-values as values\n",
    "    dict_plot_y (dict): Dictionary with dates as keys and y-values as values\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Modified dict_plot_x, dict_plot_y dictionaries, and bin_dict_mean (nested dictionary)\n",
    "    \"\"\"\n",
    "        \n",
    "    # Creating copies to avoid modifying the originals directly\n",
    "    modified_dict_x = {k: v.copy() for k, v in dict_plot_x.items()}\n",
    "    modified_dict_y = {k: v.copy() for k, v in dict_plot_y.items()}\n",
    "    \n",
    "    for i in range(0, len(dates_list)):\n",
    "        current_date = dates_list[i]\n",
    "        \n",
    "        if current_date in non_cont_dates:\n",
    "            # Discarding values in dict_plot_x[current_date] that are >= 24 and also corresponding values in dict_plot_y[current_date]\n",
    "            indices_to_remove = [idx for idx, x_val in enumerate(modified_dict_x[current_date]) if x_val >= 24]\n",
    "            \n",
    "            # Removing these values from both x and y arrays (in reverse order to avoid index shifts)\n",
    "            for idx in sorted(indices_to_remove, reverse=True):\n",
    "                modified_dict_x[current_date].pop(idx)\n",
    "                modified_dict_y[current_date].pop(idx)\n",
    "\n",
    "            print(f\"Warning: Discarding values >=24 for the this date {current_date} as it is listed in non_cont_dates.\")\n",
    "        else:\n",
    "            #Checking if this isn't the last date\n",
    "            if i + 1 < len(dates_list):\n",
    "                next_date = dates_list[i+1]\n",
    "                print(current_date, next_date)\n",
    "                # Finding indices where x values are >= 24\n",
    "                indices_to_move = [idx for idx, x_val in enumerate(modified_dict_x[current_date]) if x_val >= 24]\n",
    "                \n",
    "                if indices_to_move:  # Only process if there are values to move\n",
    "                    #Values to be moved\n",
    "                    x_values_to_move = [modified_dict_x[current_date][idx] - 24 for idx in indices_to_move]  # Subtract 24\n",
    "                    y_values_to_move = [modified_dict_y[current_date][idx] for idx in indices_to_move]\n",
    "                    \n",
    "                    #Adding these values to the next day's data\n",
    "                    modified_dict_x[next_date] = x_values_to_move + modified_dict_x[next_date]\n",
    "                    modified_dict_y[next_date] = y_values_to_move + modified_dict_y[next_date]\n",
    "                    \n",
    "                    #Removing these values from the current day (in reverse order to avoid index shifts)\n",
    "                    for idx in sorted(indices_to_move, reverse=True):\n",
    "                        modified_dict_x[current_date].pop(idx)\n",
    "                        modified_dict_y[current_date].pop(idx)\n",
    "            else:\n",
    "                #This is the last date, so we can't move values to the next day\n",
    "                print(f\"Warning: Discarding values >=24 for the last date {current_date} as there's no next day.\")\n",
    "                indices_to_remove = [idx for idx, x_val in enumerate(modified_dict_x[current_date]) if x_val >= 24]\n",
    "                \n",
    "                #Remove these values (in reverse order to avoid index shifts)\n",
    "                for idx in sorted(indices_to_remove, reverse=True):\n",
    "                    modified_dict_x[current_date].pop(idx)\n",
    "                    modified_dict_y[current_date].pop(idx)\n",
    "    \n",
    "    # Creating binned data for each date\n",
    "    bin_dict_mean = {}\n",
    "    \n",
    "    # Processing each date separately\n",
    "    for date in dates_list:\n",
    "        x_val = modified_dict_x[date]\n",
    "        y_val = modified_dict_y[date]\n",
    "        \n",
    "        # Creating bin dictionary for this date\n",
    "        bin_dict = {}\n",
    "        \n",
    "        # Creating bins\n",
    "        bin_arr = np.arange(0, 24.25, 0.25) #CAUTION: this has to be updated incase a different binning value is chosen\n",
    "        \n",
    "        for i in range(0, len(bin_arr) - 1):\n",
    "            # Creating the key for the dictionary\n",
    "            key = str(bin_arr[i]) + '_' + str(bin_arr[i+1])\n",
    "            \n",
    "            # Initializing an empty list for this key\n",
    "            templst = []\n",
    "            bin_dict[key] = templst\n",
    "            \n",
    "            # Iterating over x_val, append to templst if condition is met\n",
    "            for j in range(0, len(x_val)):\n",
    "                if x_val[j] >= bin_arr[i] and x_val[j] < bin_arr[i+1] and x_val[j] < bin_arr[-1]:\n",
    "                    # Appending y_val[j] directly to the list in the dictionary\n",
    "                    bin_dict[key].append(y_val[j])\n",
    "                if x_val[j] >= bin_arr[-1]:\n",
    "                    print(f\"Date {date}: not appending value at time: {x_val[j]}\")\n",
    "        \n",
    "        # Converting lists to numpy arrays\n",
    "        for key in bin_dict:\n",
    "            bin_dict[key] = np.array(bin_dict[key])\n",
    "        \n",
    "        # Calculating means for this date's bins\n",
    "        date_bin_dict_mean = {}\n",
    "        for key in bin_dict:\n",
    "            if len(bin_dict[key]) == 0:\n",
    "                date_bin_dict_mean[key] = np.nan\n",
    "            elif np.all(np.isnan(bin_dict[key])):\n",
    "                # List only has nan values\n",
    "                date_bin_dict_mean[key] = np.nan\n",
    "            elif ~np.all(np.isnan(bin_dict[key])) and len(bin_dict[key]) != 0:\n",
    "                # List is not empty and contains non-nan values\n",
    "                date_bin_dict_mean[key] = np.nanmean(bin_dict[key]) \n",
    "            else:\n",
    "                print(f'Date {date}: -5000 has been appended')  # Debug statement\n",
    "                date_bin_dict_mean[key] = -5000\n",
    "        \n",
    "        # Adding this date's bin means to the overall dictionary\n",
    "        bin_dict_mean[date] = date_bin_dict_mean\n",
    "    \n",
    "    return modified_dict_x, modified_dict_y, bin_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a271ee9-07fa-404f-a15d-22acbdd9424a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process each measure\n",
    "non_cont_dates = []  #for asd_001: [\"09_3_24_n2_10_3_24_d\"] #for hc_002: [] \n",
    "eda_dict_plot_x, eda_dict_plot_y, eda_dict_bin = process_time_values(non_cont_dates, eda_dates, eda_dict_plot_x, eda_dict_plot_y)\n",
    "pulse_rate_dict_plot_x, pulse_rate_dict_plot_y, pulse_rate_dict_bin = process_time_values(non_cont_dates, pulse_rate_dates, pulse_rate_dict_plot_x, pulse_rate_dict_plot_y)\n",
    "prv_dict_plot_x, prv_dict_plot_y, prv_dict_bin = process_time_values(non_cont_dates, prv_dates, prv_dict_plot_x, prv_dict_plot_y)\n",
    "resp_rate_dict_plot_x, resp_rate_dict_plot_y, resp_rate_dict_bin = process_time_values(non_cont_dates, resp_rate_dates, resp_rate_dict_plot_x, resp_rate_dict_plot_y)\n",
    "temp_dict_plot_x, temp_dict_plot_y, temp_dict_bin = process_time_values(non_cont_dates, temp_dates, temp_dict_plot_x, temp_dict_plot_y)\n",
    "step_dict_plot_x, step_dict_plot_y, step_dict_bin = process_time_values(non_cont_dates, step_dates, step_dict_plot_x, step_dict_plot_y)\n",
    "acc_std_dict_plot_x, acc_std_dict_plot_y, acc_std_dict_bin = process_time_values(non_cont_dates, acc_std_dates, acc_std_dict_plot_x, acc_std_dict_plot_y)\n",
    "activity_dict_plot_x, activity_dict_plot_y, activity_dict_bin = process_time_values(non_cont_dates, activity_dates, activity_dict_plot_x, activity_dict_plot_y)\n",
    "met_dict_plot_x, met_dict_plot_y, met_dict_bin = process_time_values(non_cont_dates, met_dates, met_dict_plot_x, met_dict_plot_y)\n",
    "wearing_det_dict_plot_x, wearing_det_dict_plot_y, wearing_det_dict_bin = process_time_values(non_cont_dates, wearing_det_dates, wearing_det_dict_plot_x, wearing_det_dict_plot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f42cd-1d36-4ac6-8a77-a26b80f251ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per day correlograms\n",
    "#need to collect each binned objective measure into separate dictionaries each containing one measure for all days \n",
    "\n",
    "list_meas = [eda_dict_bin, pulse_rate_dict_bin, prv_dict_bin, resp_rate_dict_bin, temp_dict_bin, step_dict_bin, acc_std_dict_bin, activity_dict_bin, met_dict_bin, wearing_det_dict_bin]\n",
    "\n",
    "meas = {}\n",
    "\n",
    "list_name_meas = ['eda', 'pulse_rate', 'pulse_rate_variability', 'resp_rate', \n",
    "                  'temp', 'steps', 'acc_std_dev', 'activity', \n",
    "                  'met', 'wearing_det']\n",
    "\n",
    "list_name_meas1 = ['eda_dict_bin', 'pulse_rate_dict_bin', 'prv_dict_bin', 'resp_rate_dict_bin', \n",
    "                  'temp_dict_bin', 'step_dict_bin', 'acc_std_dict_bin', 'activity_dict_bin', \n",
    "                  'met_dict_bin', 'wearing_det_dict_bin']\n",
    "\n",
    "dict_meas = {}\n",
    "\n",
    "for item_name, item in zip(list_name_meas, list_meas):\n",
    "    dict_meas[item_name] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e37705-1736-45de-bedf-29c6aa3ebd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_res = {}\n",
    "for subfolder in os.listdir(mainfolder):\n",
    "    if subfolder.endswith('d'):\n",
    "        for i in range(len(list_meas)):\n",
    "            if subfolder in list_meas[i].keys():\n",
    "                #print('yes ', subfolder,  list_name_meas[i])\n",
    "                filt_dict = {key: value for key, value in list_meas[i][subfolder].items() if not np.isnan(value)}\n",
    "                if subfolder not in norm_res:\n",
    "                        norm_res[subfolder] = {}\n",
    "                if list_name_meas[i] not in norm_res[subfolder]:\n",
    "                        norm_res[subfolder][list_name_meas[i]] = {}\n",
    "                    \n",
    "                if len(filt_dict) >= 3:\n",
    "                    #print(list_name_meas[i])\n",
    "                    stat, p_value = shapiro(list(filt_dict.values())) #sometimes issues warning saying range 0 and so reslt can be inaccurate. But couldn't figre out which measre as this warning does not appear consistently\n",
    "                    #print(stat, p_value)                  \n",
    "\n",
    "                    norm_res[subfolder][list_name_meas[i]]['length of data (without nans)'] = len(list(filt_dict.values()))\n",
    "                    norm_res[subfolder][list_name_meas[i]]['stat'] = stat\n",
    "                    norm_res[subfolder][list_name_meas[i]]['p_value'] = p_value\n",
    "                    norm_res[subfolder][list_name_meas[i]]['full data'] = list_meas[i][subfolder]\n",
    "                else:\n",
    "                    norm_res[subfolder][list_name_meas[i]]['length of data (without nans)'] = len(list(filt_dict.values()))\n",
    "                    norm_res[subfolder][list_name_meas[i]]['stat'] = None\n",
    "                    norm_res[subfolder][list_name_meas[i]]['p_value'] = None  # Not enough data for the test\n",
    "                    norm_res[subfolder][list_name_meas[i]]['full data'] = list_meas[i][subfolder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f324b7-bd39-4e8d-8a27-ca7770a2b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: in case plots don't load right away, this is a good debug strategy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1f255-0eca-47a7-8a95-e81883976b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional imports if not already executed\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64bdfb7-26ad-4778-9c0d-7a3fbf76338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot all days data on the same graph sequentially\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8af9e-7162-4121-b9b5-c24fb92e0594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#conda install anaconda::mpld3 -> didn't work, gave error. discard\n",
    "#!pip install mpld3 -> no need; did not work\n",
    "#run below if plotly not available\n",
    "!pip install plotly matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6a47a-2c11-42d6-870a-00ab037eedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional imports if not imported already\n",
    "import matplotlib\n",
    "print(matplotlib.__version__)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b0987-402c-49dc-be58-7126a129e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactive plotting and incorporating dates and days of the week\n",
    "\n",
    "# Function to get date and weekday label\n",
    "def giv_date_and_day(reqYear, reqMonth, reqDay):\n",
    "    date_obj = datetime(reqYear, reqMonth, reqDay)\n",
    "    return date_obj.strftime(\"%Y-%m-%d\\n%a\")  # Multi-line label: Date + weekday abbrev\n",
    "\n",
    "# Prepare figure for 3 stacked subplots with shared x-axis\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1, shared_xaxes=True,\n",
    "    subplot_titles=(\n",
    "        \"Subjective Dimensions (Day-stacked)\",\n",
    "        \"Objective Measures (Day-stacked), set 1\",\n",
    "        \"Objective Measures (Day-stacked), set 2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Keys and colors\n",
    "dim_keys = [f\"dim_{i}_{name}\" for i, name in enumerate([\n",
    "    \"wakefullness\", \"boredom\", \"sensory_avoidance\", \"social avoidance\",\n",
    "    \"physical tension\", \"scenario_anxiety\", \"rumination\", \"stress\", \"pain\"\n",
    "], start=1)]\n",
    "core_physio_keys = ['eda', 'pulse_rate', 'resp_rate', 'pulse_rate_variability', 'temp']\n",
    "movement_keys = ['steps', 'acc_std_dev', 'activity', 'met', 'wearing_det']\n",
    "colors = ['blue', 'orange', 'green', 'grey', 'purple', 'brown', 'pink', 'red', 'olive']\n",
    "\n",
    "x_offset = 0\n",
    "day_boundaries = []\n",
    "day_labels = []\n",
    "special_bins = [(\"12.0_12.25\", \"12:00\"), (\"18.0_18.25\", \"18:00\")]\n",
    "\n",
    "partDates = list(norm_res.keys())\n",
    "\n",
    "for day_idx, day_key in enumerate(partDates):\n",
    "    day_data = norm_res[day_key]\n",
    "    first_dim = list(day_data.keys())[0]\n",
    "    bins = list(day_data[first_dim]['full data'].keys())\n",
    "    x_vals = np.arange(len(bins)) + x_offset\n",
    "\n",
    "    # Extract date info from day key to generate tick label\n",
    "    reqYear = 2000 + int(day_key.split('_')[-2])\n",
    "    reqMonth = int(day_key.split('_')[-3])\n",
    "    reqDay = int(day_key.split('_')[-4])\n",
    "    day_label = giv_date_and_day(reqYear, reqMonth, reqDay)\n",
    "\n",
    "    day_boundaries.append(x_vals[-1])  # position of day boundary for ticks\n",
    "    print('day_boundaries = ', day_boundaries)\n",
    "    day_labels.append(day_label)       # label string for that position\n",
    "\n",
    "    # Plot dim_keys\n",
    "    for i, dim in enumerate(dim_keys):\n",
    "        if dim in day_data:\n",
    "            y_raw = list(day_data[dim]['full data'].values())\n",
    "            y_vals = [np.nan if v is None or (isinstance(v, float) and np.isnan(v)) or v == -5000 else v for v in y_raw]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_vals, y=y_vals, mode='lines',\n",
    "                name=dim, legendgroup=dim, showlegend=(day_idx == 6),\n",
    "                line=dict(color=colors[i % len(colors)], width=1.5)), row=1, col=1)\n",
    "\n",
    "    # Plot core_physio_keys\n",
    "    for i, ph in enumerate(core_physio_keys):\n",
    "        if ph in day_data and \"full data\" in day_data[ph]:\n",
    "            y_raw = list(day_data[ph]['full data'].values())\n",
    "            y_vals = [np.nan if v is None or (isinstance(v, float) and np.isnan(v)) or v == -5000 else v for v in y_raw]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_vals, y=y_vals, mode='lines',\n",
    "                name=ph, legendgroup=ph, showlegend=(day_idx == 6),\n",
    "                line=dict(color=colors[i % len(colors)], width=1.5)), row=2, col=1)\n",
    "    \n",
    "    # Plot movement_keys\n",
    "    for i, ph in enumerate(movement_keys):\n",
    "        if ph in day_data and \"full data\" in day_data[ph]:\n",
    "            y_raw = list(day_data[ph]['full data'].values())\n",
    "            y_vals = [np.nan if v is None or (isinstance(v, float) and np.isnan(v)) or v == -5000 else v for v in y_raw]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_vals, y=y_vals, mode='lines',\n",
    "                name=ph, legendgroup=ph, showlegend=(day_idx == 6),\n",
    "                line=dict(color=colors[i % len(colors)], width=1.5)), row=3, col=1)\n",
    "    \n",
    "    # Vertical dashed lines and annotations for special bins\n",
    "    for special_bin, label in special_bins:\n",
    "        if special_bin in bins:\n",
    "            idx = bins.index(special_bin)\n",
    "            xpos = x_vals[idx]\n",
    "            fig.add_shape(type=\"line\", x0=xpos, x1=xpos, y0=0, y1=1, yref=\"paper\",\n",
    "                          line=dict(color=\"grey\", width=1, dash=\"dot\"))\n",
    "            fig.add_annotation(x=xpos, y=1.05, yref=\"paper\", text=label,\n",
    "                               showarrow=False, textangle=90, yanchor='bottom')\n",
    "\n",
    "    x_offset += len(bins)\n",
    "    print('x_offset = ', x_offset)\n",
    "\n",
    "# Dashed black vertical day boundary lines\n",
    "for boundary in day_boundaries:\n",
    "    fig.add_shape(type=\"line\", x0=boundary, x1=boundary, y0=0, y1=1, yref=\"paper\",\n",
    "                  line=dict(color=\"black\", width=1, dash=\"dash\"))\n",
    "\n",
    "participant_title_list = mainfolder.split(\"\\\\\")[-1].split('_')[0:3]\n",
    "participant_title = '_'.join(participant_title_list)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    width=1200,\n",
    "    title_text=\"Participant \" +  participant_title,\n",
    "    legend=dict(yanchor=\"middle\", y=0.9, xanchor='left', x=1.05),\n",
    "   \n",
    "    xaxis3=dict(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=day_boundaries,\n",
    "        ticktext=day_labels,\n",
    "        tickangle=-45,\n",
    "        tickfont=dict(size=10),\n",
    "        showgrid=True,\n",
    "        zeroline=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Axis labels\n",
    "fig.update_xaxes(title_text=\"Time (concatenated bins across days)\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Value\")\n",
    "\n",
    "# Save interactive html\n",
    "#modify focus_folder as required\n",
    "focus_folder = r'dataclouds_CAM_LMU\\Stream_HC_002\\all_days'\n",
    "\n",
    "fig.write_html(os.path.join(focus_folder, \"updated_all_days_stacked_fifteen_min_timestitch_interactive_plot_w_days_.html\"))\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd12f78-34fd-4de3-a03b-ee38655def42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
