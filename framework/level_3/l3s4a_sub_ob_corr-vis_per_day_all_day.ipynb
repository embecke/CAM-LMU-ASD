{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c01d18f-013b-490a-9c0a-22506f3ec0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nl3s4a: per day and all day correlation and other associated analyses of subjective and objective measures (TET and wristband derived respectively)\\n\\nPART - 1\\n\\nCombination of l3s2 and l3s3 for subjective and objective measures (TET and wristband per min aggr data derived respectively)\\n\\n1. Load the script with subjective dimensions like in l3s2 until the big dictionary step.\\n2. Similarly, load per min agg objective values like in l3s3 until the big dictionary step.\\n3. Combine the two dictionaries by matching the key values (the days)\\n4. Correlate all of them with one another and output into corellogram for per day and all day and both with fdr correction - they should all be matched by the same 15 minute bins\\n\\n\\nPART - 2\\n\\nAny other useful analyses and visualisations\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "l3s4a: per day and all day correlation and other associated analyses of subjective and objective measures (TET and wristband derived respectively)\n",
    "\n",
    "PART - 1\n",
    "\n",
    "Combination of l3s2 and l3s3 for subjective and objective measures (TET and wristband per min aggr data derived respectively)\n",
    "\n",
    "1. Load the script with subjective dimensions like in l3s2 until the big dictionary step.\n",
    "2. Similarly, load per min agg objective values like in l3s3 until the big dictionary step.\n",
    "3. Combine the two dictionaries by matching the key values (the days)\n",
    "4. Correlate all of them with one another and output into corellogram for per day and all day and both with fdr correction - they should all be matched by the same 15 minute bins\n",
    "\n",
    "\n",
    "PART - 2\n",
    "\n",
    "Any other useful analyses and visualisations\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26de2d23-55fb-4e3a-b003-cde69a825654",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'l2script_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultitest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multipletests\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01ml2script_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m giv_x_y_vals, give_binned_vals, give_binned_vals_category\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'l2script_functions'"
     ]
    }
   ],
   "source": [
    "#Imports and major functions\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import kendalltau\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "\n",
    "from l2script_functions import giv_x_y_vals, give_binned_vals, give_binned_vals_category\n",
    "\n",
    "import warnings\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "define binning function for objective measures\n",
    "\"\"\"\n",
    "def give_binned_vals_obj_meas(df, obj_meas, hour, half_hour, quarter_hour, timezone, category_yn = False):\n",
    "    \n",
    "    bin_dict = {}\n",
    "    bin_dict_scl = {}\n",
    "    bin_dict_scr = {}\n",
    "\n",
    "    #only for objective measures, include the extra hour of data (if in cet)\n",
    "    if category_yn:\n",
    "        bin_arr = np.arange(0,25,6)\n",
    "    elif hour: #hour seperation\n",
    "        bin_arr = np.arange(0,25)\n",
    "    elif half_hour: #half hour seperation\n",
    "        bin_arr = np.arange(0,24.5, 0.5)\n",
    "    elif quarter_hour: #quarter hour seperation\n",
    "        bin_arr = np.arange(0,24.25, 0.25)\n",
    "    # Defining the time zones\n",
    "    utc_zone = pytz.utc\n",
    "    req_zone = pytz.timezone(timezone)\n",
    "    \n",
    "    #if aggr_p_min data, time conversion block (add an extra column to the dataframe with required timezone timestamps)\n",
    "    def from_isoutc_to_req(iso_timestamp):\n",
    "            #Parsing the ISO 8601 timestamp into a datetime object\n",
    "            utc_time = datetime.fromisoformat(iso_timestamp.replace(\"Z\", \"+00:00\"))\n",
    "    \n",
    "            #Converting from utc to required (cet) time\n",
    "            req_time = utc_time.astimezone(req_zone)\n",
    "            #print(req_time, type(req_time))\n",
    "    \n",
    "            return req_time \n",
    "\n",
    "    # Apply the conversion function to the 'utc_timestamps' column and create a new column 'converted_timestamps'\n",
    "    df['converted_timestamps'] = df['timestamp_iso'].apply(from_isoutc_to_req)\n",
    "\n",
    "    first_day = df['converted_timestamps'].iloc[0].day \n",
    "\n",
    "    x_val = df['converted_timestamps'].apply(\n",
    "        lambda x: (24 + int(str(x).split()[1].split(':')[0]) + int(str(x).split()[1].split(':')[1])/60) \n",
    "        if x.day > first_day \n",
    "        else (int(str(x).split()[1].split(':')[0]) + int(str(x).split()[1].split(':')[1])/60)\n",
    "    ).tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    #Optional, not used so far\n",
    "    #range of permissible eda range\n",
    "    min_val = 0.05\n",
    "    max_val = 60\n",
    "    #yes but what about scl specifically??? -> haven't found a source talking about scl permissible ranges so for now set this aside\n",
    "    \"\"\"\n",
    "    if obj_meas == 'eda':\n",
    "        y_val = df['eda_scl_usiemens'].tolist()\n",
    "    elif obj_meas == 'pulse_rate':\n",
    "        y_val = df['pulse_rate_bpm'].tolist()\n",
    "    elif obj_meas == 'prv':\n",
    "        y_val = df['prv_rmssd_ms'].tolist()\n",
    "    elif obj_meas == 'resp_rate':\n",
    "        y_val = df['respiratory_rate_brpm'].tolist()\n",
    "    elif obj_meas == 'temp':\n",
    "        y_val = df['temperature_celsius'].tolist()\n",
    "    elif obj_meas == 'step_count':\n",
    "        y_val = df['step_counts'].tolist()\n",
    "    elif obj_meas == 'acc_std':\n",
    "        y_val = df['accelerometers_std_g'].tolist()\n",
    "    elif obj_meas == 'activity_counts':\n",
    "        y_val = df['activity_counts'].tolist()\n",
    "    elif obj_meas == 'met':\n",
    "        y_val = df['met'].tolist() \n",
    "    elif obj_meas == 'wearing_det':\n",
    "        y_val = df['wearing_detection_percentage'].tolist()\n",
    "    ###Optional: include measures after review\n",
    "    \n",
    "    for i in range(0, len(bin_arr) - 1):\n",
    "            #Create the key for the dictionary\n",
    "            key = str(bin_arr[i]) + '_' + str(bin_arr[i+1])\n",
    "    \n",
    "            #Initialize an empty list for this key\n",
    "            templst = []\n",
    "            bin_dict[key] = templst\n",
    "        \n",
    "            #Iterate over x_val, append to templst if condition is met\n",
    "            for j in range(0, len(x_val)):\n",
    "                if x_val[j] >= bin_arr[i] and x_val[j] < bin_arr[i+1] and x_val[j]<bin_arr[-1]: \n",
    "                    #Appending y_val[j] directly to the list in the dictionary\n",
    "                    bin_dict[key].append(y_val[j])\n",
    "                if x_val[j]>=bin_arr[-1]:\n",
    "                    #print(\"not appending value at time: \", x_val[j])\n",
    "                    continue\n",
    "\n",
    "    #for conversion of lists to numpy arrays\n",
    "    for key in bin_dict:\n",
    "            bin_dict[key] = np.array(bin_dict[key])\n",
    "            #print(bin_dict)\n",
    "\n",
    "    bin_dict_mean = {}\n",
    "    for key in bin_dict:\n",
    "            if np.all(np.isnan(bin_dict[key])):\n",
    "                #print('list only has nan values')\n",
    "                bin_dict_mean[key] = np.nan\n",
    "            elif ~np.all(np.isnan(bin_dict[key])) and len(bin_dict[key])!=0:\n",
    "                #print('list is not empty')\n",
    "                bin_dict_mean[key] = np.nanmean(bin_dict[key]) \n",
    "            else:\n",
    "                print('-5000 has been appended') #this print statement added to debug if there is ever a situation where this would happen (technically it shouldn't)\n",
    "                bin_dict_mean[key] = -5000\n",
    "\n",
    "    return  df['converted_timestamps'], x_val, y_val, bin_dict_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18ee4b-7484-4ace-89c2-545442ceae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some more common global variable definitions\n",
    "\n",
    "folder1 = 'empatica'\n",
    "folder2 = 'saved_figures'\n",
    "\n",
    "folder11 = 'aggr_p_min'\n",
    "folder12 = 'avro_files'\n",
    "folder13 = 'avro2csv'\n",
    "folder14 = 'preprocessed_files_debug'\n",
    "folder141 = 'data_preproc_debug'\n",
    "\n",
    "ger = True #if country of data collection is Germany, true, else (if UK, false) -> because language of TET questions and other input data config changes by country\n",
    "timezone = 'Europe/Berlin' # 'utc' #default timezone; enter required timezone if different\n",
    "mainfolder = input('enter participant folder: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182be21-1641-451d-8e60-8a2df4cbcb8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_TET_x1, dict_TET_y1,  x_new1, y_new1, x1, y1 = giv_x_y_vals(mainfolder, 'q1', ger) #has duplicates (asd_001)\n",
    "dict_TET_x2, dict_TET_y2,  x_new2, y_new2, x2, y2 = giv_x_y_vals(mainfolder, 'q2', ger) #has duplicates and days with missing data (asd_001)\n",
    "dict_TET_x3, dict_TET_y3,  x_new3, y_new3, x3, y3 = giv_x_y_vals(mainfolder, 'q3', ger) #has duplicates (hc_002) #has days with missing data (asd_001)\n",
    "dict_TET_x4, dict_TET_y4,  x_new4, y_new4, x4, y4 = giv_x_y_vals(mainfolder, 'q4', ger) #has days with missing data (asd_001)\n",
    "dict_TET_x5, dict_TET_y5,  x_new5, y_new5, x5, y5 = giv_x_y_vals(mainfolder, 'q5', ger) #has duplicates and days with missing data (asd_001)\n",
    "dict_TET_x6, dict_TET_y6,  x_new6, y_new6, x6, y6 = giv_x_y_vals(mainfolder, 'q6', ger) #has duplicates (hc_002) #has days with missing data (asd_001)\n",
    "dict_TET_x7, dict_TET_y7,  x_new7, y_new7, x7, y7 = giv_x_y_vals(mainfolder, 'q7', ger)\n",
    "dict_TET_x8, dict_TET_y8,  x_new8, y_new8, x8, y8 = giv_x_y_vals(mainfolder, 'q8', ger) #has duplicates and days with missing data (asd_001) -> but for the day that it had duplicate data (15_3_24_n7_16_3_24_d) the data was identical so all good\n",
    "dict_TET_x9, dict_TET_y9,  x_new9, y_new9, x9, y9 = giv_x_y_vals(mainfolder, 'q9', ger) #has duplicates (hc_002) #has days with missing data (asd_001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4218ce-9b8f-4324-85c3-23090fd940a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, obtain the binned dictionaries for each dimension\n",
    "\n",
    "dict_TET_x = [dict_TET_x1, dict_TET_x2, dict_TET_x3, dict_TET_x4, dict_TET_x5, dict_TET_x6, dict_TET_x7, dict_TET_x8, dict_TET_x9]\n",
    "dict_TET_y = [dict_TET_y1, dict_TET_y2, dict_TET_y3, dict_TET_y4, dict_TET_y5, dict_TET_y6, dict_TET_y7, dict_TET_y8, dict_TET_y9]\n",
    "\n",
    "dim_q = {}\n",
    "\n",
    "#pairing up the names of the dimensions for added info\n",
    "dim_names = ['wakefullness', 'boredom', 'sensory_avoidance', 'social avoidance', 'physical tension', 'scenario_anxiety', 'rumination', 'stress', 'pain'] #can add 'personalised_dimension' as and when it becomes applicable. But most of the times, data is not available\n",
    "for i in range(9):\n",
    "    dim_q[f'dim_{i+1}_{dim_names[i]}'] = {}\n",
    "    for key in dict_TET_x[i]:\n",
    "        x_val = dict_TET_x[i][key] * 6\n",
    "        y_val = dict_TET_y[i][key]\n",
    "        dim_q[f'dim_{i+1}_{dim_names[i]}'][key] = give_binned_vals(x_val, y_val, '15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc293431-942a-48a6-87dd-04feb4a1c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for each day, assess each dimension array for normality using shapiro wilk and generate correlograms (pearson and kendall). add an indication for every significant test statistic (p<0.05)\n",
    "\n",
    "#step 1: for every day, every available dimension assessed for normality and reported in a single dictionary or dataframe\n",
    "\n",
    "norm_res = {}\n",
    "for subfolder in os.listdir(mainfolder):\n",
    "    if subfolder.endswith('d'):\n",
    "        for i in range(9):\n",
    "            if subfolder in dim_q[f'dim_{i+1}_{dim_names[i]}'].keys():\n",
    "                #print('yes ', subfolder,  f'dim_{i+1}_{dim_names[i]}')\n",
    "                filt_dict = {key: value for key, value in dim_q[f'dim_{i+1}_{dim_names[i]}'][subfolder].items() if value != -5000}\n",
    "                #print(filt_dict)\n",
    "                if subfolder not in norm_res:\n",
    "                        norm_res[subfolder] = {}\n",
    "                if f'dim_{i+1}_{dim_names[i]}' not in norm_res[subfolder]:\n",
    "                        norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}'] = {}\n",
    "                    \n",
    "                if len(filt_dict) >= 3:\n",
    "                    stat, p_value = shapiro(list(filt_dict.values()))\n",
    "                    #print(stat, p_value)\n",
    "                    \n",
    "                    \n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['length of data'] = len(list(filt_dict.values()))\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['stat'] = stat\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['p_value'] = p_value\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['full data'] = dim_q[f'dim_{i+1}_{dim_names[i]}'][subfolder]\n",
    "                else:\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['length of data'] = len(list(filt_dict.values()))\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['stat'] = None\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['p_value'] = None  # Not enough data for the test\n",
    "                    norm_res[subfolder][f'dim_{i+1}_{dim_names[i]}']['full data'] = dim_q[f'dim_{i+1}_{dim_names[i]}'][subfolder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff2e9a-01ec-4486-bfb7-c7e35b9a0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_dict_plot_x = {}\n",
    "eda_dict_plot_y = {}\n",
    "eda_dict_bin = {}\n",
    "pulse_rate_dict_plot_x = {}\n",
    "pulse_rate_dict_plot_y = {}\n",
    "pulse_rate_dict_bin = {}\n",
    "prv_dict_plot_x = {}\n",
    "prv_dict_plot_y = {}\n",
    "prv_dict_bin = {}\n",
    "resp_rate_dict_plot_x = {}\n",
    "resp_rate_dict_plot_y = {}\n",
    "resp_rate_dict_bin = {}\n",
    "temp_dict_plot_x = {}\n",
    "temp_dict_plot_y = {}\n",
    "temp_dict_bin = {}\n",
    "step_dict_plot_x = {}\n",
    "step_dict_plot_y = {}\n",
    "step_dict_bin = {}\n",
    "acc_std_dict_plot_x = {}\n",
    "acc_std_dict_plot_y = {}\n",
    "acc_std_dict_bin = {}\n",
    "activity_dict_plot_x = {}\n",
    "activity_dict_plot_y = {}\n",
    "activity_dict_bin = {}\n",
    "met_dict_plot_x = {}\n",
    "met_dict_plot_y = {}\n",
    "met_dict_bin = {}\n",
    "wearing_det_dict_plot_x = {}\n",
    "wearing_det_dict_plot_y = {}\n",
    "wearing_det_dict_bin = {}\n",
    "\n",
    "\n",
    "\n",
    "#in the lines below, take out \"_converted_timestamp,\" variables after timestamp verification check\n",
    "eda_converted_timestamp = {}\n",
    "pulse_rate_converted_timestamp = {}\n",
    "prv_converted_timestamp = {}\n",
    "resp_rate_converted_timestamp = {}\n",
    "temp_converted_timestamp = {}\n",
    "step_converted_timestamp = {}\n",
    "acc_converted_timestamp = {}\n",
    "activity_converted_timestamp = {}\n",
    "met_converted_timestamp = {}\n",
    "wearing_det_converted_timestamp = {}\n",
    "\n",
    "#storing the dates for which the variables are recorded. Required for time-stitching\n",
    "eda_dates = []\n",
    "pulse_rate_dates = []\n",
    "prv_dates = []\n",
    "resp_rate_dates = []\n",
    "temp_dates = []\n",
    "step_dates = []\n",
    "acc_std_dates = []\n",
    "activity_dates = []\n",
    "met_dates = []\n",
    "wearing_det_dates = []\n",
    "\n",
    "\n",
    "for subfolder in os.listdir(mainfolder):\n",
    "    if subfolder.endswith('d') and os.path.exists(os.path.join(mainfolder, subfolder, folder1, folder11)):\n",
    "        print(subfolder)\n",
    "        for file in os.listdir(os.path.join(mainfolder, subfolder, folder1, folder11)):\n",
    "            if file.endswith('eda.csv'):\n",
    "                eda_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                eda_converted_timestamp[subfolder], eda_dict_plot_x[subfolder], eda_dict_plot_y[subfolder], eda_dict_bin[subfolder] = give_binned_vals_obj_meas(eda_df, 'eda', False, False, True, timezone)\n",
    "                eda_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('pulse-rate.csv'):\n",
    "                pulse_rate_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                pulse_rate_converted_timestamp[subfolder], pulse_rate_dict_plot_x[subfolder], pulse_rate_dict_plot_y[subfolder], pulse_rate_dict_bin[subfolder] = give_binned_vals_obj_meas(pulse_rate_df, 'pulse_rate', False, False, True, timezone)\n",
    "                pulse_rate_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('prv.csv'):\n",
    "                prv_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                prv_converted_timestamp[subfolder], prv_dict_plot_x[subfolder], prv_dict_plot_y[subfolder], prv_dict_bin[subfolder] = give_binned_vals_obj_meas(prv_df, 'prv', False, False, True, timezone)\n",
    "                prv_dates.append(subfolder)\n",
    "                \n",
    "            elif file.endswith('respiratory-rate.csv'):\n",
    "                resp_rate_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                resp_rate_converted_timestamp[subfolder], resp_rate_dict_plot_x[subfolder], resp_rate_dict_plot_y[subfolder], resp_rate_dict_bin[subfolder] = give_binned_vals_obj_meas(resp_rate_df, 'resp_rate', False, False, True, timezone)\n",
    "                resp_rate_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('temperature.csv'):\n",
    "                temp_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                temp_converted_timestamp[subfolder], temp_dict_plot_x[subfolder], temp_dict_plot_y[subfolder], temp_dict_bin[subfolder] = give_binned_vals_obj_meas(temp_df, 'temp', False, False, True, timezone)\n",
    "                temp_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('step-counts.csv'):\n",
    "                step_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                step_converted_timestamp[subfolder], step_dict_plot_x[subfolder], step_dict_plot_y[subfolder], step_dict_bin[subfolder] = give_binned_vals_obj_meas(step_df, 'step_count', False, False, True, timezone)\n",
    "                step_dates.append(subfolder)\n",
    "                \n",
    "            elif file.endswith('accelerometers-std.csv'):\n",
    "                acc_std_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                acc_converted_timestamp[subfolder], acc_std_dict_plot_x[subfolder], acc_std_dict_plot_y[subfolder], acc_std_dict_bin[subfolder] = give_binned_vals_obj_meas(acc_std_df, 'acc_std', False, False, True, timezone)\n",
    "                acc_std_dates.append(subfolder)\n",
    "                \n",
    "            elif file.endswith('activity-counts.csv'):\n",
    "                activity_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                activity_converted_timestamp[subfolder], activity_dict_plot_x[subfolder], activity_dict_plot_y[subfolder], activity_dict_bin[subfolder] = give_binned_vals_obj_meas(activity_df, 'activity_counts', False, False, True, timezone)\n",
    "                activity_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('met.csv'):\n",
    "                met_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                met_converted_timestamp[subfolder], met_dict_plot_x[subfolder], met_dict_plot_y[subfolder], met_dict_bin[subfolder] = give_binned_vals_obj_meas(met_df, 'met', False, False, True, timezone)\n",
    "                met_dates.append(subfolder)\n",
    "            \n",
    "            elif file.endswith('wearing-detection.csv'):\n",
    "                wearing_det_df = pd.read_csv(os.path.join(mainfolder, subfolder, folder1, folder11, file))\n",
    "                wearing_det_converted_timestamp[subfolder], wearing_det_dict_plot_x[subfolder], wearing_det_dict_plot_y[subfolder], wearing_det_dict_bin[subfolder] = give_binned_vals_obj_meas(wearing_det_df, 'wearing_det', False, False, True, timezone)\n",
    "                wearing_det_dates.append(subfolder)                \n",
    "    #(df, obj_meas, hour, half_hour, quarter_hour, timezone):\n",
    "    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ca9b8-d9df-4d1b-a118-2fb5a8f584ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional function for timestitch\n",
    "def process_time_values(non_cont_dates, dates_list, dict_plot_x, dict_plot_y):\n",
    "    \"\"\"\n",
    "    Process time values to move data points with x >= 24 to the next day.\n",
    "    Also creates binned data and calculates bin means for each date.\n",
    "    \n",
    "    Parameters:\n",
    "    non_cont_dates (list): List of dates that are not continuous i.e; dates where the next night is not the very next date but further off. \n",
    "    dates_list (list): List of dates for the specific measure\n",
    "    dict_plot_x (dict): Dictionary with dates as keys and x-values as values\n",
    "    dict_plot_y (dict): Dictionary with dates as keys and y-values as values\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Modified dict_plot_x, dict_plot_y dictionaries, and bin_dict_mean (nested dictionary)\n",
    "    \"\"\"\n",
    "        \n",
    "    # Creating copies to avoid modifying the originals directly\n",
    "    modified_dict_x = {k: v.copy() for k, v in dict_plot_x.items()}\n",
    "    modified_dict_y = {k: v.copy() for k, v in dict_plot_y.items()}\n",
    "    \n",
    "    for i in range(0, len(dates_list)):\n",
    "        current_date = dates_list[i]\n",
    "        \n",
    "        if current_date in non_cont_dates:\n",
    "            # Discarding values in dict_plot_x[current_date] that are >= 24 and also corresponding values in dict_plot_y[current_date]\n",
    "            indices_to_remove = [idx for idx, x_val in enumerate(modified_dict_x[current_date]) if x_val >= 24]\n",
    "            \n",
    "            # Removing these values from both x and y arrays (in reverse order to avoid index shifts)\n",
    "            for idx in sorted(indices_to_remove, reverse=True):\n",
    "                modified_dict_x[current_date].pop(idx)\n",
    "                modified_dict_y[current_date].pop(idx)\n",
    "\n",
    "            print(f\"Warning: Discarding values >=24 for the this date {current_date} as it is listed in non_cont_dates.\")\n",
    "        else:\n",
    "            #Checking if this isn't the last date\n",
    "            if i + 1 < len(dates_list):\n",
    "                next_date = dates_list[i+1]\n",
    "                print(current_date, next_date)\n",
    "                # Finding indices where x values are >= 24\n",
    "                indices_to_move = [idx for idx, x_val in enumerate(modified_dict_x[current_date]) if x_val >= 24]\n",
    "                \n",
    "                if indices_to_move:  # Only process if there are values to move\n",
    "                    #Values to be moved\n",
    "                    x_values_to_move = [modified_dict_x[current_date][idx] - 24 for idx in indices_to_move]  # Subtract 24\n",
    "                    y_values_to_move = [modified_dict_y[current_date][idx] for idx in indices_to_move]\n",
    "                    \n",
    "                    #Adding these values to the next day's data\n",
    "                    modified_dict_x[next_date] = x_values_to_move + modified_dict_x[next_date]\n",
    "                    modified_dict_y[next_date] = y_values_to_move + modified_dict_y[next_date]\n",
    "                    \n",
    "                    #Removing these values from the current day (in reverse order to avoid index shifts)\n",
    "                    for idx in sorted(indices_to_move, reverse=True):\n",
    "                        modified_dict_x[current_date].pop(idx)\n",
    "                        modified_dict_y[current_date].pop(idx)\n",
    "            else:\n",
    "                #This is the last date, so we can't move values to the next day\n",
    "                print(f\"Warning: Discarding values >=24 for the last date {current_date} as there's no next day.\")\n",
    "                indices_to_remove = [idx for idx, x_val in enumerate(modified_dict_x[current_date]) if x_val >= 24]\n",
    "                \n",
    "                #Remove these values (in reverse order to avoid index shifts)\n",
    "                for idx in sorted(indices_to_remove, reverse=True):\n",
    "                    modified_dict_x[current_date].pop(idx)\n",
    "                    modified_dict_y[current_date].pop(idx)\n",
    "    \n",
    "    # Creating binned data for each date\n",
    "    bin_dict_mean = {}\n",
    "    \n",
    "    # Processing each date separately\n",
    "    for date in dates_list:\n",
    "        x_val = modified_dict_x[date]\n",
    "        y_val = modified_dict_y[date]\n",
    "        \n",
    "        # Creating bin dictionary for this date\n",
    "        bin_dict = {}\n",
    "        \n",
    "        # Creating bins\n",
    "        bin_arr = np.arange(0, 24.25, 0.25) #CAUTION: this has to be updated incase a different binning value is chosen\n",
    "        \n",
    "        for i in range(0, len(bin_arr) - 1):\n",
    "            # Creating the key for the dictionary\n",
    "            key = str(bin_arr[i]) + '_' + str(bin_arr[i+1])\n",
    "            \n",
    "            # Initializing an empty list for this key\n",
    "            templst = []\n",
    "            bin_dict[key] = templst\n",
    "            \n",
    "            # Iterating over x_val, append to templst if condition is met\n",
    "            for j in range(0, len(x_val)):\n",
    "                if x_val[j] >= bin_arr[i] and x_val[j] < bin_arr[i+1] and x_val[j] < bin_arr[-1]:\n",
    "                    # Appending y_val[j] directly to the list in the dictionary\n",
    "                    bin_dict[key].append(y_val[j])\n",
    "                if x_val[j] >= bin_arr[-1]:\n",
    "                    print(f\"Date {date}: not appending value at time: {x_val[j]}\")\n",
    "        \n",
    "        # Converting lists to numpy arrays\n",
    "        for key in bin_dict:\n",
    "            bin_dict[key] = np.array(bin_dict[key])\n",
    "        \n",
    "        # Calculating means for this date's bins\n",
    "        date_bin_dict_mean = {}\n",
    "        for key in bin_dict:\n",
    "            if len(bin_dict[key]) == 0:\n",
    "                date_bin_dict_mean[key] = np.nan\n",
    "            elif np.all(np.isnan(bin_dict[key])):\n",
    "                # List only has nan values\n",
    "                date_bin_dict_mean[key] = np.nan\n",
    "            elif ~np.all(np.isnan(bin_dict[key])) and len(bin_dict[key]) != 0:\n",
    "                # List is not empty and contains non-nan values\n",
    "                date_bin_dict_mean[key] = np.nanmean(bin_dict[key]) \n",
    "            else:\n",
    "                print(f'Date {date}: -5000 has been appended')  # Debug statement\n",
    "                date_bin_dict_mean[key] = -5000\n",
    "        \n",
    "        # Adding this date's bin means to the overall dictionary\n",
    "        bin_dict_mean[date] = date_bin_dict_mean\n",
    "    \n",
    "    return modified_dict_x, modified_dict_y, bin_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a271ee9-07fa-404f-a15d-22acbdd9424a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process each measure\n",
    "non_cont_dates = []  #for asd_001: [\"09_3_24_n2_10_3_24_d\"] #for hc_002: [] \n",
    "eda_dict_plot_x, eda_dict_plot_y, eda_dict_bin = process_time_values(non_cont_dates, eda_dates, eda_dict_plot_x, eda_dict_plot_y)\n",
    "pulse_rate_dict_plot_x, pulse_rate_dict_plot_y, pulse_rate_dict_bin = process_time_values(non_cont_dates, pulse_rate_dates, pulse_rate_dict_plot_x, pulse_rate_dict_plot_y)\n",
    "prv_dict_plot_x, prv_dict_plot_y, prv_dict_bin = process_time_values(non_cont_dates, prv_dates, prv_dict_plot_x, prv_dict_plot_y)\n",
    "resp_rate_dict_plot_x, resp_rate_dict_plot_y, resp_rate_dict_bin = process_time_values(non_cont_dates, resp_rate_dates, resp_rate_dict_plot_x, resp_rate_dict_plot_y)\n",
    "temp_dict_plot_x, temp_dict_plot_y, temp_dict_bin = process_time_values(non_cont_dates, temp_dates, temp_dict_plot_x, temp_dict_plot_y)\n",
    "step_dict_plot_x, step_dict_plot_y, step_dict_bin = process_time_values(non_cont_dates, step_dates, step_dict_plot_x, step_dict_plot_y)\n",
    "acc_std_dict_plot_x, acc_std_dict_plot_y, acc_std_dict_bin = process_time_values(non_cont_dates, acc_std_dates, acc_std_dict_plot_x, acc_std_dict_plot_y)\n",
    "activity_dict_plot_x, activity_dict_plot_y, activity_dict_bin = process_time_values(non_cont_dates, activity_dates, activity_dict_plot_x, activity_dict_plot_y)\n",
    "met_dict_plot_x, met_dict_plot_y, met_dict_bin = process_time_values(non_cont_dates, met_dates, met_dict_plot_x, met_dict_plot_y)\n",
    "wearing_det_dict_plot_x, wearing_det_dict_plot_y, wearing_det_dict_bin = process_time_values(non_cont_dates, wearing_det_dates, wearing_det_dict_plot_x, wearing_det_dict_plot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f42cd-1d36-4ac6-8a77-a26b80f251ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per day correlograms\n",
    "#need to collect each binned objective measure into separate dictionaries each containing one measure for all days \n",
    "\n",
    "list_meas = [eda_dict_bin, pulse_rate_dict_bin, prv_dict_bin, resp_rate_dict_bin, temp_dict_bin, step_dict_bin, acc_std_dict_bin, activity_dict_bin, met_dict_bin, wearing_det_dict_bin]\n",
    "\n",
    "meas = {}\n",
    "\n",
    "list_name_meas = ['eda', 'pulse_rate', 'pulse_rate_variability', 'resp_rate', \n",
    "                  'temp', 'steps', 'acc_std_dev', 'activity', \n",
    "                  'met', 'wearing_det']\n",
    "\n",
    "list_name_meas1 = ['eda_dict_bin', 'pulse_rate_dict_bin', 'prv_dict_bin', 'resp_rate_dict_bin', \n",
    "                  'temp_dict_bin', 'step_dict_bin', 'acc_std_dict_bin', 'activity_dict_bin', \n",
    "                  'met_dict_bin', 'wearing_det_dict_bin']\n",
    "\n",
    "dict_meas = {}\n",
    "\n",
    "for item_name, item in zip(list_name_meas, list_meas):\n",
    "    dict_meas[item_name] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e37705-1736-45de-bedf-29c6aa3ebd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_res = {}\n",
    "for subfolder in os.listdir(mainfolder):\n",
    "    if subfolder.endswith('d'):\n",
    "        for i in range(len(list_meas)):\n",
    "            if subfolder in list_meas[i].keys():\n",
    "                #print('yes ', subfolder,  list_name_meas[i])\n",
    "                filt_dict = {key: value for key, value in list_meas[i][subfolder].items() if not np.isnan(value)}\n",
    "                if subfolder not in norm_res:\n",
    "                        norm_res[subfolder] = {}\n",
    "                if list_name_meas[i] not in norm_res[subfolder]:\n",
    "                        norm_res[subfolder][list_name_meas[i]] = {}\n",
    "                    \n",
    "                if len(filt_dict) >= 3:\n",
    "                    #print(list_name_meas[i])\n",
    "                    stat, p_value = shapiro(list(filt_dict.values())) #sometimes issues warning saying range 0 and so reslt can be inaccurate. But couldn't figre out which measre as this warning does not appear consistently\n",
    "                    #print(stat, p_value)                  \n",
    "\n",
    "                    norm_res[subfolder][list_name_meas[i]]['length of data (without nans)'] = len(list(filt_dict.values()))\n",
    "                    norm_res[subfolder][list_name_meas[i]]['stat'] = stat\n",
    "                    norm_res[subfolder][list_name_meas[i]]['p_value'] = p_value\n",
    "                    norm_res[subfolder][list_name_meas[i]]['full data'] = list_meas[i][subfolder]\n",
    "                else:\n",
    "                    norm_res[subfolder][list_name_meas[i]]['length of data (without nans)'] = len(list(filt_dict.values()))\n",
    "                    norm_res[subfolder][list_name_meas[i]]['stat'] = None\n",
    "                    norm_res[subfolder][list_name_meas[i]]['p_value'] = None  # Not enough data for the test\n",
    "                    norm_res[subfolder][list_name_meas[i]]['full data'] = list_meas[i][subfolder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f324b7-bd39-4e8d-8a27-ca7770a2b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: in case plots don't load right away, this is a good debug strategy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1f255-0eca-47a7-8a95-e81883976b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional imports if not already executed\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64bdfb7-26ad-4778-9c0d-7a3fbf76338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot all days data on the same graph sequentially\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8af9e-7162-4121-b9b5-c24fb92e0594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#conda install anaconda::mpld3 -> didn't work, gave error. discard\n",
    "#!pip install mpld3 -> no need; did not work\n",
    "#run below if plotly not available\n",
    "!pip install plotly matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6a47a-2c11-42d6-870a-00ab037eedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional imports if not imported already\n",
    "import matplotlib\n",
    "print(matplotlib.__version__)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b0987-402c-49dc-be58-7126a129e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactive plotting and incorporating dates and days of the week\n",
    "\n",
    "# Function to get date and weekday label\n",
    "def giv_date_and_day(reqYear, reqMonth, reqDay):\n",
    "    date_obj = datetime(reqYear, reqMonth, reqDay)\n",
    "    return date_obj.strftime(\"%Y-%m-%d\\n%a\")  # Multi-line label: Date + weekday abbrev\n",
    "\n",
    "# Prepare figure for 3 stacked subplots with shared x-axis\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1, shared_xaxes=True,\n",
    "    subplot_titles=(\n",
    "        \"Subjective Dimensions (Day-stacked)\",\n",
    "        \"Objective Measures (Day-stacked), set 1\",\n",
    "        \"Objective Measures (Day-stacked), set 2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Keys and colors\n",
    "dim_keys = [f\"dim_{i}_{name}\" for i, name in enumerate([\n",
    "    \"wakefullness\", \"boredom\", \"sensory_avoidance\", \"social avoidance\",\n",
    "    \"physical tension\", \"scenario_anxiety\", \"rumination\", \"stress\", \"pain\"\n",
    "], start=1)]\n",
    "core_physio_keys = ['eda', 'pulse_rate', 'resp_rate', 'pulse_rate_variability', 'temp']\n",
    "movement_keys = ['steps', 'acc_std_dev', 'activity', 'met', 'wearing_det']\n",
    "colors = ['blue', 'orange', 'green', 'grey', 'purple', 'brown', 'pink', 'red', 'olive']\n",
    "\n",
    "x_offset = 0\n",
    "day_boundaries = []\n",
    "day_labels = []\n",
    "special_bins = [(\"12.0_12.25\", \"12:00\"), (\"18.0_18.25\", \"18:00\")]\n",
    "\n",
    "partDates = list(norm_res.keys())\n",
    "\n",
    "for day_idx, day_key in enumerate(partDates):\n",
    "    day_data = norm_res[day_key]\n",
    "    first_dim = list(day_data.keys())[0]\n",
    "    bins = list(day_data[first_dim]['full data'].keys())\n",
    "    x_vals = np.arange(len(bins)) + x_offset\n",
    "\n",
    "    # Extract date info from day key to generate tick label\n",
    "    reqYear = 2000 + int(day_key.split('_')[-2])\n",
    "    reqMonth = int(day_key.split('_')[-3])\n",
    "    reqDay = int(day_key.split('_')[-4])\n",
    "    day_label = giv_date_and_day(reqYear, reqMonth, reqDay)\n",
    "\n",
    "    day_boundaries.append(x_vals[-1])  # position of day boundary for ticks\n",
    "    print('day_boundaries = ', day_boundaries)\n",
    "    day_labels.append(day_label)       # label string for that position\n",
    "\n",
    "    # Plot dim_keys\n",
    "    for i, dim in enumerate(dim_keys):\n",
    "        if dim in day_data:\n",
    "            y_raw = list(day_data[dim]['full data'].values())\n",
    "            y_vals = [np.nan if v is None or (isinstance(v, float) and np.isnan(v)) or v == -5000 else v for v in y_raw]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_vals, y=y_vals, mode='lines',\n",
    "                name=dim, legendgroup=dim, showlegend=(day_idx == 6),\n",
    "                line=dict(color=colors[i % len(colors)], width=1.5)), row=1, col=1)\n",
    "\n",
    "    # Plot core_physio_keys\n",
    "    for i, ph in enumerate(core_physio_keys):\n",
    "        if ph in day_data and \"full data\" in day_data[ph]:\n",
    "            y_raw = list(day_data[ph]['full data'].values())\n",
    "            y_vals = [np.nan if v is None or (isinstance(v, float) and np.isnan(v)) or v == -5000 else v for v in y_raw]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_vals, y=y_vals, mode='lines',\n",
    "                name=ph, legendgroup=ph, showlegend=(day_idx == 6),\n",
    "                line=dict(color=colors[i % len(colors)], width=1.5)), row=2, col=1)\n",
    "    \n",
    "    # Plot movement_keys\n",
    "    for i, ph in enumerate(movement_keys):\n",
    "        if ph in day_data and \"full data\" in day_data[ph]:\n",
    "            y_raw = list(day_data[ph]['full data'].values())\n",
    "            y_vals = [np.nan if v is None or (isinstance(v, float) and np.isnan(v)) or v == -5000 else v for v in y_raw]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_vals, y=y_vals, mode='lines',\n",
    "                name=ph, legendgroup=ph, showlegend=(day_idx == 6),\n",
    "                line=dict(color=colors[i % len(colors)], width=1.5)), row=3, col=1)\n",
    "    \n",
    "    # Vertical dashed lines and annotations for special bins\n",
    "    for special_bin, label in special_bins:\n",
    "        if special_bin in bins:\n",
    "            idx = bins.index(special_bin)\n",
    "            xpos = x_vals[idx]\n",
    "            fig.add_shape(type=\"line\", x0=xpos, x1=xpos, y0=0, y1=1, yref=\"paper\",\n",
    "                          line=dict(color=\"grey\", width=1, dash=\"dot\"))\n",
    "            fig.add_annotation(x=xpos, y=1.05, yref=\"paper\", text=label,\n",
    "                               showarrow=False, textangle=90, yanchor='bottom')\n",
    "\n",
    "    x_offset += len(bins)\n",
    "    print('x_offset = ', x_offset)\n",
    "\n",
    "# Dashed black vertical day boundary lines\n",
    "for boundary in day_boundaries:\n",
    "    fig.add_shape(type=\"line\", x0=boundary, x1=boundary, y0=0, y1=1, yref=\"paper\",\n",
    "                  line=dict(color=\"black\", width=1, dash=\"dash\"))\n",
    "\n",
    "participant_title_list = mainfolder.split(\"\\\\\")[-1].split('_')[0:3]\n",
    "participant_title = '_'.join(participant_title_list)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    width=1200,\n",
    "    title_text=\"Participant \" +  participant_title,\n",
    "    legend=dict(yanchor=\"middle\", y=0.9, xanchor='left', x=1.05),\n",
    "   \n",
    "    xaxis3=dict(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=day_boundaries,\n",
    "        ticktext=day_labels,\n",
    "        tickangle=-45,\n",
    "        tickfont=dict(size=10),\n",
    "        showgrid=True,\n",
    "        zeroline=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Axis labels\n",
    "fig.update_xaxes(title_text=\"Time (concatenated bins across days)\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Value\")\n",
    "\n",
    "# Save interactive html\n",
    "#modify focus_folder as required\n",
    "focus_folder = r'dataclouds_CAM_LMU\\Stream_HC_002\\all_days'\n",
    "\n",
    "fig.write_html(os.path.join(focus_folder, \"updated_all_days_stacked_fifteen_min_timestitch_interactive_plot_w_days_.html\"))\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c55db5-3b05-48d6-922b-5b0f991e0f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PER DAY CORELLOGRAM - CAN MODIFY THE INCLUDED SUBJECTIVE/OBJECTIVE PARAMETERS FOR FOCUSED VISUALISATION\n",
    "\n",
    "SAVES FIGURES, INTEGRATES ALL INFO IN EACH CELL\n",
    "\n",
    "ALSO TRIES DATACLOUDS\n",
    "\n",
    "Choose from the measures listed below if you only want to visualise and check correlations of a subset of measures. List the excluded ones in \"measures_to_pop\" and uncomment the line\n",
    "\n",
    "#['dim_1_wakefullness', 'dim_2_boredom', 'dim_3_sensory_avoidance', 'dim_4_social avoidance', 'dim_5_physical tension', 'dim_6_scenario_anxiety', 'dim_7_rumination', 'dim_8_stress', 'dim_9_pain', 'eda', 'pulse_rate', 'pulse_rate_variability', 'resp_rate', 'temp', 'steps', 'acc_std_dev', 'activity', 'met', 'wearing_det']\n",
    "measures_to_pop = ['dim_2_boredom', 'dim_3_sensory_avoidance', 'dim_4_social avoidance', 'dim_6_scenario_anxiety', 'temp', 'steps', 'activity', 'met', 'wearing_det'] \n",
    "\n",
    "#specify the folder for savefig line in daily datacloud function\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def giv_date_and_day(reqYear, reqMonth, reqDay):\n",
    "    date_obj = datetime(reqYear, reqMonth, reqDay)\n",
    "    return date_obj.strftime(\"%Y-%m-%d\\n%a\")  \n",
    "    \n",
    "def create_daily_datacloud_matrix(df, subfolder, mainfolder, folder2, kendall_mat, kendall_pval_mat, kendall_pval_mat_fdr, kendall_len_mat, min_points=1):\n",
    "    \"\"\"\n",
    "    Create a matrix of scatter plots (dataclouds) for daily correlation analysis\n",
    "    Now uses FDR-corrected p-values for significance\n",
    "    \"\"\"\n",
    "    #special_var: dictionary of variables that have a certain upper limit (inferred from their daily traces and plots). If they are included in the dictionary below, the plots will have these x and y upper limits. If not the upper limits will go upto 4 (the range of TET magnitude values)\n",
    "    special_var = {'eda': 45, 'pulse_rate':250, 'pulse_rate_variability':300, 'resp_rate':30, 'acc_std_dev':1} #this can also vary from participant to participant. Verify and update values\n",
    "    # Filter out columns with all NaN or insufficient data\n",
    "    valid_columns = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dropna().shape[0] >= 3:  # Need at least 3 points\n",
    "            valid_columns.append(col)\n",
    "    \n",
    "    if len(valid_columns) < 2:\n",
    "        print(f\"Insufficient data for dataclouds on {subfolder}\")\n",
    "        return\n",
    "    \n",
    "    n_vars = len(valid_columns)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_vars, n_vars, figsize=(4*n_vars, 4*n_vars))\n",
    "    suptitle = f'Daily Datacloud for {subfolder}'\n",
    "    fig.suptitle(suptitle, fontsize=16, y=0.98)\n",
    "    \n",
    "    # If only one variable, make axes 2D for consistent indexing\n",
    "    if n_vars == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_vars == 2:\n",
    "        axes = axes.reshape(2, 2)\n",
    "    \n",
    "    for i, dim1 in enumerate(valid_columns):\n",
    "        for j, dim2 in enumerate(valid_columns):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Get filtered data for this pair\n",
    "            filt_data = df[[dim1, dim2]].dropna()\n",
    "            \n",
    "            if len(filt_data) >= 3:\n",
    "                if i == j:  # Diagonal - leave blank\n",
    "                    ax.text(0.5, 0.5, f'Diagonal\\n', \n",
    "                           transform=ax.transAxes, ha='center', va='center',\n",
    "                           fontsize=10, color='red')\n",
    "                    ax.set_xlabel(dim1, fontsize=9)\n",
    "                    ax.set_ylabel(dim2, fontsize=9)\n",
    "                elif i>j:  # Off-diagonal - show scatter plot\n",
    "                    x_data = filt_data[dim1]\n",
    "                    y_data = filt_data[dim2]\n",
    "                    # Create scatter plot\n",
    "                    ax.scatter(x_data, y_data, alpha=0.6, s=20, color='steelblue')\n",
    "                    if dim1 in special_var.keys():\n",
    "                        for key, val in special_var.items():\n",
    "                            if dim1 == key:\n",
    "                                ax.set_xlim(0, val)\n",
    "                    else:\n",
    "                        ax.set_xlim(0, 4)\n",
    "                    if dim2 in special_var.keys():\n",
    "                        for key, val in special_var.items():\n",
    "                            if dim2 == key:\n",
    "                                ax.set_ylim(0, val)\n",
    "                    else:\n",
    "                        ax.set_ylim(0, 4)\n",
    "                    # Get correlation statistics from existing matrices\n",
    "                    if len(filt_data) >= min_points and not np.isnan(kendall_mat.loc[dim1, dim2]):\n",
    "                        corr_ken = kendall_mat.loc[dim1, dim2]\n",
    "                        p_ken_raw = kendall_pval_mat.loc[dim1, dim2]\n",
    "                        p_ken_fdr = kendall_pval_mat_fdr.loc[dim1, dim2]\n",
    "                        n_points = int(kendall_len_mat.loc[dim1, dim2])\n",
    "                        \n",
    "                        # Add trend line if FDR-corrected correlation is significant and reasonably strong\n",
    "                        if not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05 and abs(corr_ken) > 0.2:\n",
    "                            # Fit linear trend line\n",
    "                            z = np.polyfit(x_data, y_data, 1)\n",
    "                            p = np.poly1d(z)\n",
    "                            ax.plot(x_data.sort_values(), p(x_data.sort_values()), \n",
    "                                   \"r--\", alpha=0.8, linewidth=1.5)\n",
    "                        \n",
    "                        # Add correlation info with both raw and FDR p-values\n",
    "                        fdr_sig = \"*\" if (not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05) else \"\"\n",
    "                        raw_sig = \"*\" if p_ken_raw < 0.05 else \"\"\n",
    "                        \n",
    "                        annotation_text = f'tau={corr_ken:.3f}{fdr_sig}\\n'\n",
    "                        annotation_text += f'p_raw={p_ken_raw:.3f}{raw_sig}\\n'\n",
    "                        if not np.isnan(p_ken_fdr):\n",
    "                            annotation_text += f'p_fdr={p_ken_fdr:.3f}{fdr_sig}\\n'\n",
    "                        else:\n",
    "                            annotation_text += f'p_fdr=n/a\\n'\n",
    "                        annotation_text += f'n={n_points}'\n",
    "                        \n",
    "                        ax.text(0.05, 0.95, annotation_text,\n",
    "                               transform=ax.transAxes, fontsize=7,\n",
    "                               verticalalignment='top',\n",
    "                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                    \n",
    "                    ax.set_xlabel(dim1, fontsize=9)\n",
    "                    ax.set_ylabel(dim2, fontsize=9)\n",
    "                else: \n",
    "                    # Upper triangle; need not be repeated\n",
    "                    ax.text(0.5, 0.5, f'Upper triangle\\n', \n",
    "                           transform=ax.transAxes, ha='center', va='center',\n",
    "                           fontsize=10, color='red')\n",
    "                    ax.set_xlabel(dim1, fontsize=9)\n",
    "                    ax.set_ylabel(dim2, fontsize=9)\n",
    "                    \n",
    "            else:\n",
    "                # Not enough data\n",
    "                ax.text(0.5, 0.5, f'Insufficient data\\n(n={len(filt_data)})', \n",
    "                       transform=ax.transAxes, ha='center', va='center',\n",
    "                       fontsize=10, color='red')\n",
    "                ax.set_xlabel(dim1, fontsize=9)\n",
    "                ax.set_ylabel(dim2, fontsize=9)\n",
    "                if dim1 == special_var:\n",
    "                    ax.set_xlim(0, 45)\n",
    "                else:\n",
    "                    ax.set_xlim(0, 4)\n",
    "                if dim2 == special_var:\n",
    "                    ax.set_ylim(0, 45)\n",
    "                else:\n",
    "                    ax.set_ylim(0, 4)\n",
    "            \n",
    "            # Clean up axes\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #specify the folder to save the figures. If folder does not exist, create it. \n",
    "    focused_folder = folder2 \n",
    "    plt.savefig(os.path.join(focused_folder, f\"v2_focused_daily_datacloud_fdr_{subfolder}_timestitch.png\"),bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_daily_condensed_datacloud_matrix(df, subfolder, mainfolder, folder2, kendall_mat, kendall_pval_mat, kendall_pval_mat_fdr, kendall_len_mat, min_points=1):\n",
    "    \"\"\"\n",
    "    Optional added function to plot the same plots as above but in order of their correlation strength from highest to lowest. Can also add (copy-paste) the x and y limit logic from the previous function to this one if using\n",
    "    Create a condensed matrix showing only significant daily correlations (lower triangle)\n",
    "    Now uses FDR-corrected p-values for significance\n",
    "    \"\"\"\n",
    "    # Filter out columns with all NaN or insufficient data\n",
    "    valid_columns = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dropna().shape[0] >= 3:\n",
    "            valid_columns.append(col)\n",
    "    \n",
    "    if len(valid_columns) < 2:\n",
    "        print(f\"Need at least 2 variables for correlation dataclouds on {subfolder}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate number of unique pairs\n",
    "    n_vars = len(valid_columns)\n",
    "    n_unique_pairs = n_vars * (n_vars - 1) // 2\n",
    "    \n",
    "    if n_unique_pairs == 0:\n",
    "        print(f\"Need at least 2 variables for correlation dataclouds on {subfolder}\")\n",
    "        return\n",
    "    \n",
    "    # Determine subplot layout\n",
    "    cols = min(4, n_unique_pairs)  # Max 4 columns\n",
    "    rows = (n_unique_pairs + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    suptitle = f'Daily Significant Correlations (FDR-corrected) - Dataclouds for {subfolder}'\n",
    "    fig.suptitle(suptitle, fontsize=14)\n",
    "    \n",
    "    # Make axes iterable\n",
    "    if n_unique_pairs == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if hasattr(axes, 'flatten') else axes\n",
    "    \n",
    "    plot_idx = 0\n",
    "    significant_pairs = []\n",
    "    \n",
    "    # Collect all pairs first (lower triangle)\n",
    "    for i, dim1 in enumerate(valid_columns):\n",
    "        for j, dim2 in enumerate(valid_columns):\n",
    "            if i <= j:  # Only lower triangle\n",
    "                continue\n",
    "                \n",
    "            # Get filtered data\n",
    "            filt_data = df[[dim1, dim2]].dropna()\n",
    "            \n",
    "            if len(filt_data) >= min_points and not np.isnan(kendall_mat.loc[dim1, dim2]):\n",
    "                corr_ken = kendall_mat.loc[dim1, dim2]\n",
    "                p_ken_raw = kendall_pval_mat.loc[dim1, dim2]\n",
    "                p_ken_fdr = kendall_pval_mat_fdr.loc[dim1, dim2]\n",
    "                n_points = int(kendall_len_mat.loc[dim1, dim2])\n",
    "                significant_pairs.append((dim1, dim2, filt_data, corr_ken, p_ken_raw, p_ken_fdr, n_points))\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    significant_pairs.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "    \n",
    "    # Plot significant pairs\n",
    "    for dim1, dim2, filt_data, corr_ken, p_ken_raw, p_ken_fdr, n_points in significant_pairs[:len(axes)]:\n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        x_data = filt_data[dim1]\n",
    "        y_data = filt_data[dim2]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        ax.scatter(x_data, y_data, alpha=0.6, s=30, color='steelblue')\n",
    "        \n",
    "        # Add trend line for FDR-significant correlations\n",
    "        if not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05:\n",
    "            z = np.polyfit(x_data, y_data, 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_trend = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "            ax.plot(x_trend, p(x_trend), \"r-\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Add statistics with FDR information\n",
    "        fdr_significance = \"*\" if (not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05) else \"\"\n",
    "        raw_significance = \"*\" if p_ken_raw < 0.05 else \"\"\n",
    "        \n",
    "        title_text = f'{dim1} vs {dim2}\\ntau={corr_ken:.3f}{fdr_significance}'\n",
    "        subtitle_text = f'p_raw={p_ken_raw:.3f}{raw_significance}'\n",
    "        if not np.isnan(p_ken_fdr):\n",
    "            subtitle_text += f', p_fdr={p_ken_fdr:.3f}{fdr_significance}'\n",
    "        subtitle_text += f', n={n_points}'\n",
    "        \n",
    "        ax.set_title(title_text, fontsize=10, fontweight='bold')\n",
    "        ax.text(0.5, -0.15, subtitle_text, transform=ax.transAxes, \n",
    "                ha='center', fontsize=8, style='italic')\n",
    "        \n",
    "        ax.set_xlabel(dim1, fontsize=9)\n",
    "        ax.set_ylabel(dim2, fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(plot_idx, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(mainfolder, subfolder, folder2, f\"filtered_daily_significant_dataclouds_fdr_{subfolder}_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "# Main per-day analysis loop with integrated dataclouds\n",
    "kendall_mat_dict = {}\n",
    "kendall_pval_mat_dict = {}\n",
    "kendall_pval_mat_fdr_dict = {}\n",
    "kendall_len_mat_dict = {}\n",
    "shapiro_pval_mat_dict = {}\n",
    "\n",
    "for subfolder in norm_res.keys():\n",
    "    if subfolder == '25_3_24_n17_26_3_24_d': #this if statement is for a specific participant (ASD_001) because there was no recorded subjective TET data recorded for this day. Modufy this if necessary for other participants if applicable or keep it as is, code executes regardless\n",
    "        continue\n",
    "    else:\n",
    "        dim_dict = {}\n",
    "        for dim_qx in norm_res[subfolder].keys():\n",
    "            if dim_qx in measures_to_pop:\n",
    "                continue\n",
    "                \n",
    "            dim_dict[dim_qx] = norm_res[subfolder][dim_qx]['full data']\n",
    "        df = pd.DataFrame(dim_dict) \n",
    "        #now need a corellogram such that for values =-5000 are turned to NaN and dropped only for the two columns being correlated. Kendall correlation using scipy needs to exceute\n",
    "        df.replace(-5000, np.nan, inplace=True)\n",
    "        #shapiro_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns) - optional, not necessary\n",
    "        shapiro_pval_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "        kendall_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "        kendall_pval_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "        kendall_len_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "    \n",
    "        #for fdr correction\n",
    "        kendall_pval_mat_fdr = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "        \n",
    "        for dim1 in df.columns:\n",
    "            for dim2 in df.columns:\n",
    "                filt_data = df[[dim1, dim2]].dropna()\n",
    "    \n",
    "                if len(filt_data)>=3:\n",
    "                    stat1, pval1 = shapiro(filt_data[dim1])\n",
    "                    stat2, pval2 = shapiro(filt_data[dim2])\n",
    "                    corr_ken, p_ken = kendalltau(filt_data[dim1], filt_data[dim2])\n",
    "                    if pval1 > 0.05 and pval2 > 0.05:\n",
    "                        shapiro_pval_mat.loc[dim1, dim2] = 1\n",
    "                    else:\n",
    "                        shapiro_pval_mat.loc[dim1, dim2] = 0\n",
    "                    kendall_mat.loc[dim1, dim2] = corr_ken\n",
    "                    kendall_pval_mat.loc[dim1,dim2] = p_ken\n",
    "                    kendall_len_mat.loc[dim1,dim2] = len(filt_data)\n",
    "                else:\n",
    "                    #not enough data to test normality or calculate kendall corr\n",
    "                    shapiro_pval_mat.loc[dim1, dim2] = np.nan\n",
    "                    kendall_mat.loc[dim1, dim2] = np.nan\n",
    "                    kendall_pval_mat.loc[dim1,dim2] = np.nan\n",
    "                    kendall_len_mat.loc[dim1,dim2] = len(filt_data)\n",
    "    \n",
    "        #Applying fdr correction\n",
    "        temp_store = []\n",
    "        pPreFdr = []\n",
    "        pos = []\n",
    "        for meas_1 in df.columns:\n",
    "            temp_store.append(meas_1)\n",
    "            for meas_2 in df.columns:\n",
    "                if meas_1 == meas_2:\n",
    "                    continue\n",
    "                elif meas_2 in temp_store:\n",
    "                    continue\n",
    "                else:\n",
    "                    if not np.isnan(kendall_mat.loc[meas_1, meas_2]):\n",
    "                        pPreFdr.append(kendall_pval_mat.loc[meas_1, meas_2])\n",
    "                        pos.append((meas_1, meas_2))\n",
    "        \n",
    "        pPostFdr = multipletests(pPreFdr, method='fdr_bh')[1]\n",
    "        \n",
    "        for (meas_1, meas_2), p_adj in zip(pos, pPostFdr):\n",
    "            kendall_pval_mat_fdr.loc[meas_1, meas_2] = p_adj\n",
    "            kendall_pval_mat_fdr.loc[meas_2, meas_1] = p_adj\n",
    "        \n",
    "        #fill diagonal and any missing values with NaN \n",
    "        for meas in df.columns:\n",
    "            kendall_pval_mat_fdr.loc[meas, meas] = np.nan\n",
    "    \n",
    "        #storing values per day\n",
    "        kendall_mat_dict[subfolder] = kendall_mat.copy()\n",
    "        kendall_pval_mat_dict[subfolder] = kendall_pval_mat.copy()\n",
    "        kendall_pval_mat_fdr_dict[subfolder] = kendall_pval_mat_fdr.copy()\n",
    "        kendall_len_mat_dict[subfolder] = kendall_len_mat.copy()\n",
    "        shapiro_pval_mat_dict[subfolder] = shapiro_pval_mat.copy()\n",
    "    \n",
    "        #creating a custom annotation matrix combining tau, p-value, and data length\n",
    "        annotations = kendall_mat.copy()\n",
    "        \n",
    "        for dim1 in df.columns:\n",
    "            for dim2 in df.columns:\n",
    "                if not np.isnan(kendall_mat.loc[dim1, dim2]):\n",
    "                    annotations.loc[dim1, dim2] = f\"tau={kendall_mat.loc[dim1, dim2]:.2f}\\n\"\n",
    "                    if kendall_pval_mat.loc[dim1, dim2] < 0.05:\n",
    "                        annotations.loc[dim1, dim2] += f\"p={kendall_pval_mat.loc[dim1, dim2]:.2f}*\\n\"\n",
    "                    else:\n",
    "                        annotations.loc[dim1, dim2] += f\"p={kendall_pval_mat.loc[dim1, dim2]:.2f}\\n\"\n",
    "                    annotations.loc[dim1, dim2] += f\"dl={int(kendall_len_mat.loc[dim1, dim2])}\\n\"\n",
    "                    annotations.loc[dim1, dim2] += f\"norm={int(shapiro_pval_mat.loc[dim1, dim2])}\"\n",
    "                else:\n",
    "                    annotations.loc[dim1, dim2] = \"\"\n",
    "    \n",
    "        # Plot the combined heatmap with custom annotations (tau, p-value, and data length)\n",
    "        plt.figure(figsize=(32, 32))\n",
    "        sns.heatmap(kendall_mat, annot=annotations, fmt=\"\", cmap='coolwarm', square=True, cbar=True, mask=np.isnan(kendall_mat), annot_kws={\"size\": 12})\n",
    "        plt.title(f'Kendall Correlation, p-value, and Data Length for {subfolder}')\n",
    "        plt.savefig(os.path.join(mainfolder, subfolder, folder2, \"subjective_objective_corr_timestitch.png\"), bbox_inches='tight', dpi=300) \n",
    "        plt.show()\n",
    "    \n",
    "        #repeating annotations and figure for fdr correction\n",
    "        #creating a custom annotation matrix combining normality test, tau, p-value, and data length\n",
    "        annotations_fdr = kendall_mat.copy()\n",
    "        for meas_1 in df.columns:\n",
    "            for meas_2 in df.columns:\n",
    "                if not np.isnan(kendall_mat.loc[meas_1, meas_2]):\n",
    "                    annotations_fdr.loc[meas_1, meas_2] = f\"tau={kendall_mat.loc[meas_1, meas_2]:.2f}\\n\" #tau\n",
    "                    if kendall_pval_mat_fdr.loc[meas_1, meas_2] < 0.05:\n",
    "                        annotations_fdr.loc[meas_1, meas_2] += f\"p={kendall_pval_mat_fdr.loc[meas_1, meas_2]:.2f}*\\n\"\n",
    "                    else:\n",
    "                        annotations_fdr.loc[meas_1, meas_2] += f\"p={kendall_pval_mat_fdr.loc[meas_1, meas_2]:.2f}\\n\"\n",
    "                    annotations_fdr.loc[meas_1, meas_2] += f\"dl={int(kendall_len_mat.loc[meas_1, meas_2])}\\n\"\n",
    "                    annotations_fdr.loc[meas_1, meas_2] += f\"norm={int(shapiro_pval_mat.loc[meas_1, meas_2])}\"\n",
    "                else:\n",
    "                    annotations_fdr.loc[meas_1, meas_2] = \"\"\n",
    "        \n",
    "        # Plot the combined heatmap with custom annotations (tau, p-value, and data length)\n",
    "        plt.figure(figsize=(32, 32))\n",
    "        sns.heatmap(kendall_mat, annot=annotations_fdr, fmt=\"\", cmap='coolwarm', square=True, cbar=True, mask=np.isnan(kendall_mat), annot_kws={\"size\": 12})\n",
    "        title = 'Kendall Correlation, p-value (fdr corrected), and Data Length for day: '+ subfolder\n",
    "        plt.title(title)\n",
    "        plt.savefig(os.path.join(mainfolder, subfolder, folder2, \"subjective_objective_corr_fdr_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        # ========== ADD DATACLOUDS FOR EACH DAY ==========\n",
    "        print(f\"Creating daily datacloud matrix for {subfolder}...\")\n",
    "        #if folder2 is different from what has already been defined, modify and assign it here\n",
    "        create_daily_datacloud_matrix(df, subfolder, mainfolder, folder2, \n",
    "                                     kendall_mat, kendall_pval_mat, kendall_pval_mat_fdr, kendall_len_mat)\n",
    "        \n",
    "        #print(f\"Creating condensed daily datacloud matrix for {subfolder}...\")\n",
    "        #create_daily_condensed_datacloud_matrix(df, subfolder, mainfolder, folder2,\n",
    "                                               #kendall_mat, kendall_pval_mat, kendall_pval_mat_fdr, kendall_len_mat)\n",
    "        \n",
    "        \n",
    "        print(f\"Completed datacloud analysis for {subfolder}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "#Next segment - stem plots of tau values with annotated p values for selected measures with focus on subjective stress TET and objective tonic EDA obtained from digital biomarker of wristband\n",
    "\n",
    "partDates = list(norm_res.keys())  # Or norm_res.keys(), as appropriate\n",
    "#for asd_001:\n",
    "#partDates.remove('25_3_24_n17_26_3_24_d')\n",
    "\n",
    "# List your pairs of interest:\n",
    "# For example: [('stress', 'eda'), ('stress', 'physical_tension'), ...]\n",
    "# For each pair to plot\n",
    "#['dim_1_wakefullness', 'dim_2_boredom', 'dim_3_sensory_avoidance', 'dim_4_social avoidance', 'dim_5_physical tension', 'dim_6_scenario_anxiety', 'dim_7_rumination', 'dim_8_stress', 'dim_9_pain', 'eda', 'pulse_rate', 'pulse_rate_variability', 'resp_rate', 'temp', 'steps', 'acc_std_dev', 'activity', 'met', 'wearing_det']\n",
    "#adjust as per participant\n",
    "meas_list = ['dim_1_wakefullness', 'dim_5_physical tension', 'dim_7_rumination', 'dim_8_stress', 'dim_9_pain', 'eda', 'pulse_rate', 'pulse_rate_variability', 'resp_rate', 'acc_std_dev']\n",
    "measure_pairs = [('dim_8_stress', meas) for meas in meas_list if meas != 'dim_8_stress']\n",
    "measure_pairs += [('eda', meas) for meas in meas_list if meas != 'eda']\n",
    "measure_pairs += [('dim_8_stress', 'eda')]\n",
    "\n",
    "for var1, var2 in measure_pairs:\n",
    "    tau_vals = []\n",
    "    p_raw_vals = []\n",
    "    p_fdr_vals = []\n",
    "    annotation_texts = []\n",
    "    tick_labels = []\n",
    "    valid_day_indices = []\n",
    "\n",
    "    for idx, day_key in enumerate(partDates):\n",
    "        try:\n",
    "            parts = day_key.split('_')\n",
    "            reqYear = 2000 + int(parts[-2])  #adjust if your data uses 1900-base years - unlikely\n",
    "            reqMonth = int(parts[-3])\n",
    "            reqDay = int(parts[-4])\n",
    "            this_label = giv_date_and_day(reqYear, reqMonth, reqDay)\n",
    "        except Exception:\n",
    "            this_label = day_key\n",
    "\n",
    "        #Retrieve correlation values\n",
    "        kendall = kendall_mat_dict[day_key]\n",
    "        pval = kendall_pval_mat_dict[day_key]\n",
    "        pval_fdr = kendall_pval_mat_fdr_dict[day_key]\n",
    "        if var1 in kendall.index and var2 in kendall.columns and not np.isnan(kendall.loc[var1, var2]):\n",
    "            tau = kendall.loc[var1, var2]\n",
    "            p = pval.loc[var1, var2]\n",
    "            p_fdr = pval_fdr.loc[var1, var2]\n",
    "            tau_vals.append(tau)\n",
    "            p_raw_vals.append(p)\n",
    "            p_fdr_vals.append(p_fdr)\n",
    "            if p_fdr < 0.05:\n",
    "                annotation_texts.append(f\"tau={tau:.2f}*\\np={p:.2g}\\np_fdr={p_fdr:.2g}\")\n",
    "            else:\n",
    "                annotation_texts.append(f\"tau={tau:.2f}\\np={p:.2g}\\np_fdr={p_fdr:.2g}\")\n",
    "            tick_labels.append(this_label)\n",
    "            valid_day_indices.append(idx)\n",
    "\n",
    "    # Now plot only days with valid data\n",
    "    fig, ax = plt.subplots(figsize=(max(10, len(valid_day_indices)*0.7), 6))\n",
    "    markerline, stemlines, baseline = ax.stem(range(len(tau_vals)), tau_vals, linefmt='g-', markerfmt='go', basefmt='r-')\n",
    "    ax.axhline(0, color='red', linestyle='dashed')\n",
    "    plt.xticks(range(len(tau_vals)), tick_labels, rotation=45, fontsize=10)\n",
    "    ax.set_ylabel(\"Correlation (Kendall's tau)\")\n",
    "    ax.set_xlabel(\"Day (date + weekday)\")\n",
    "    #for graph naming\n",
    "    if var1.startswith('dim'):\n",
    "        var1 = var1.split('_')[-1]\n",
    "    if var2.startswith('dim'):\n",
    "        var2 = var2.split('_')[-1]\n",
    "    \n",
    "    ax.set_title(f\"Per-day Correlation: {var1} vs {var2}\")\n",
    "\n",
    "    # Annotate at the top of each stick\n",
    "    for i, txt in enumerate(annotation_texts):\n",
    "        ax.text(i, tau_vals[i], txt, ha='left', va='bottom', fontsize=5, color='brown')\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    #specify focus_folder; example folder given below\n",
    "    focused_folder = r'dataclouds_CAM_LMU\\Stream_HC_002\\per_day\\focused'\n",
    "    plt.savefig(os.path.join(focused_folder, f\"{var1}_{var2}_corr_fdr_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa99e68-140d-4e55-8ae9-273c8a24635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SAME AS ABOVE BUT WITH LAGGED CROSS-CORRELATION\n",
    "\n",
    "#['dim_1_wakefullness', 'dim_2_boredom', 'dim_3_sensory_avoidance', 'dim_4_social avoidance', 'dim_5_physical tension', 'dim_6_scenario_anxiety', 'dim_7_rumination', 'dim_8_stress', 'dim_9_pain', 'eda', 'pulse_rate', 'pulse_rate_variability', 'resp_rate', 'temp', 'steps', 'acc_std_dev', 'activity', 'met', 'wearing_det']\n",
    "\n",
    "measures_to_pop = ['dim_2_boredom', 'dim_3_sensory_avoidance', 'dim_4_social avoidance', 'dim_6_scenario_anxiety', 'dim_7_rumination','dim_9_pain', 'pulse_rate', 'pulse_rate_variability', 'resp_rate', 'temp', 'steps', 'acc_std_dev', 'activity', 'met', 'wearing_det'] \n",
    "\n",
    "#All the instructions of the previous cell apply here. The only difference is the introduction of lags (optional)\n",
    "Note: Since bins are 15 minytes in length, each hour has 4 bins. Therefore if lag is in the order of hours, specify bins in the corresponding multiples of 4\n",
    "\"\"\"\n",
    "lag = -24\n",
    "\n",
    "focus_folder = r'dataclouds_CAM_LMU\\Stream_ASD_001\\per_day\\focused' #specify as per particiipant or if it is the pre-defined folder2\n",
    "folder_req = os.path.join(focus_folder, 'lag', str(lag))\n",
    "\n",
    "if not os.path.exists(folder_req):\n",
    "    os.makedirs(folder_req, exist_ok=True)\n",
    "focused_folder = folder_req\n",
    "\n",
    "def giv_date_and_day(reqYear, reqMonth, reqDay):\n",
    "    date_obj = datetime(reqYear, reqMonth, reqDay)\n",
    "    return date_obj.strftime(\"%Y-%m-%d\\n%a\") \n",
    "\n",
    "def lagged_kendall_tau(df, lag):\n",
    "    # Shift one column by lag (forward lag; negative for backward lag)\n",
    "    shifted = df.copy()\n",
    "    col1 = df.columns[0]\n",
    "    col2 = df.columns[1]\n",
    "    shifted[col1] = shifted[col1].shift(lag)\n",
    "    # Drop rows with any NaN (either initial or from shift)\n",
    "    clean = shifted.dropna(subset=[col1, col2])\n",
    "    \n",
    "    stat1, pval1, stat2, pval2, tau, p_value = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    if len(clean)>=3:\n",
    "        stat1, pval1 = shapiro(clean[col1])\n",
    "        stat2, pval2 = shapiro(clean[col2])\n",
    "        # Compute Kendall's tau\n",
    "        tau, p_value = kendalltau(clean[col1], clean[col2])\n",
    "    return stat1, pval1, stat2, pval2, tau, p_value\n",
    "\n",
    "\n",
    "def create_daily_datacloud_matrix(df, subfolder, mainfolder, kendall_mat, kendall_pval_mat, kendall_pval_mat_fdr, kendall_len_mat, lag, focused_folder, min_points=1):\n",
    "    \"\"\"\n",
    "    Create a matrix of scatter plots (dataclouds) for daily correlation analysis\n",
    "    Now uses FDR-corrected p-values for significance\n",
    "    \"\"\"\n",
    "    special_var = 'eda' #modify to dictionary with specified limits as in the previous cell if necessary. Currently this function only looks at eda\n",
    "    # Filter out columns with all NaN or insufficient data\n",
    "    valid_columns = []\n",
    "   \n",
    "    for col in df.columns:\n",
    "        if df[col].dropna().shape[0] >= 3:  # Need at least 3 points\n",
    "            valid_columns.append(col)\n",
    "    \n",
    "            \n",
    "    if len(valid_columns) < 2:\n",
    "        print(f\"Insufficient data for dataclouds on {subfolder}\")\n",
    "        return\n",
    "    \n",
    "    n_vars = len(valid_columns)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_vars, n_vars, figsize=(4*n_vars, 4*n_vars))\n",
    "    suptitle = f'Daily Datacloud for {subfolder}'\n",
    "    fig.suptitle(suptitle, fontsize=16, y=0.98)\n",
    "    \n",
    "    # If only one variable, make axes 2D for consistent indexing\n",
    "    if n_vars == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_vars == 2:\n",
    "        axes = axes.reshape(2, 2)\n",
    "    \n",
    "    for i, dim1 in enumerate(valid_columns):\n",
    "        for j, dim2 in enumerate(valid_columns):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Get filtered data for this pair\n",
    "            #filt_data = df[[dim1, dim2]].dropna()\n",
    "            shifted = df.copy()\n",
    "            #col1 = df.columns[0]\n",
    "            #col2 = df.columns[1]\n",
    "            shifted[dim1] = shifted[dim1].shift(lag)\n",
    "            # Drop rows with any NaN (either initial or from shift)\n",
    "            filt_data = shifted.dropna(subset=[dim1, dim2])\n",
    "            \n",
    "            if len(filt_data) >= 3:\n",
    "                if i == j:  # Diagonal - leave blank\n",
    "                    ax.text(0.5, 0.5, f'Diagonal\\n', \n",
    "                           transform=ax.transAxes, ha='center', va='center',\n",
    "                           fontsize=10, color='red')\n",
    "                    ax.set_xlabel(dim1+'_lag_'+str(lag), fontsize=9)\n",
    "                    ax.set_ylabel(dim2, fontsize=9)\n",
    "                else: #if i>j:  # Off-diagonal - show scatter plot #change to just else and show upper triangle too\n",
    "                    x_data = filt_data[dim1]\n",
    "                    y_data = filt_data[dim2]\n",
    "                    # Create scatter plot\n",
    "                    ax.scatter(x_data, y_data, alpha=0.6, s=20, color='steelblue')\n",
    "                    if dim1 == special_var: #here is where the upper limit is specified. modify like in previous cell if more variables with different limits are required\n",
    "                        ax.set_xlim(0, 30)\n",
    "                    else:\n",
    "                        ax.set_xlim(0, 4)\n",
    "                    if dim2 == special_var:\n",
    "                        ax.set_ylim(0, 30)\n",
    "                    else:\n",
    "                        ax.set_ylim(0, 4)\n",
    "                    # Get correlation statistics from existing matrices\n",
    "                    if len(filt_data) >= min_points and not np.isnan(kendall_mat.loc[dim1, dim2]):\n",
    "                        corr_ken = kendall_mat.loc[dim1, dim2]\n",
    "                        p_ken_raw = kendall_pval_mat.loc[dim1, dim2]\n",
    "                        p_ken_fdr = kendall_pval_mat_fdr.loc[dim1, dim2]\n",
    "                        n_points = int(kendall_len_mat.loc[dim1, dim2])\n",
    "                        \n",
    "                        # Add trend line if FDR-corrected correlation is significant and reasonably strong\n",
    "                        if not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05 and abs(corr_ken) > 0.2:\n",
    "                            # Fit linear trend line\n",
    "                            z = np.polyfit(x_data, y_data, 1)\n",
    "                            p = np.poly1d(z)\n",
    "                            ax.plot(x_data.sort_values(), p(x_data.sort_values()), \n",
    "                                   \"r--\", alpha=0.8, linewidth=1.5)\n",
    "                        \n",
    "                        # Add correlation info with both raw and FDR p-values\n",
    "                        fdr_sig = \"*\" if (not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05) else \"\"\n",
    "                        raw_sig = \"*\" if p_ken_raw < 0.05 else \"\"\n",
    "                        \n",
    "                        annotation_text = f'tau={corr_ken:.3f}{fdr_sig}\\n'\n",
    "                        annotation_text += f'p_raw={p_ken_raw:.3f}{raw_sig}\\n'\n",
    "                        if not np.isnan(p_ken_fdr):\n",
    "                            annotation_text += f'p_fdr={p_ken_fdr:.3f}{fdr_sig}\\n'\n",
    "                        else:\n",
    "                            annotation_text += f'p_fdr=n/a\\n'\n",
    "                        annotation_text += f'n={n_points}'\n",
    "                        \n",
    "                        ax.text(0.05, 0.95, annotation_text,\n",
    "                               transform=ax.transAxes, fontsize=7,\n",
    "                               verticalalignment='top',\n",
    "                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                    \n",
    "                    ax.set_xlabel(dim1+'_lag_'+str(lag), fontsize=9)\n",
    "                    ax.set_ylabel(dim2, fontsize=9)\n",
    "                \n",
    "                    \n",
    "            else:\n",
    "                # Not enough data\n",
    "                ax.text(0.5, 0.5, f'Insufficient data\\n(n={len(filt_data)})', \n",
    "                       transform=ax.transAxes, ha='center', va='center',\n",
    "                       fontsize=10, color='red')\n",
    "                ax.set_xlabel(dim1+'_lag_'+str(lag), fontsize=9)\n",
    "                ax.set_ylabel(dim2, fontsize=9)\n",
    "                if dim1 == special_var:\n",
    "                    ax.set_xlim(0, 30)\n",
    "                else:\n",
    "                    ax.set_xlim(0, 4)\n",
    "                if dim2 == special_var:\n",
    "                    ax.set_ylim(0, 30)\n",
    "                else:\n",
    "                    ax.set_ylim(0, 4)\n",
    "            \n",
    "            # Clean up axes\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    plt.savefig(os.path.join(focused_folder, f\"focused_daily_datacloud_fdr_{subfolder}_timestitch.png\"),bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_daily_condensed_datacloud_matrix(df, subfolder, mainfolder, folder2, kendall_mat, kendall_pval_mat, kendall_pval_mat_fdr, kendall_len_mat, min_points=1):\n",
    "    \"\"\"\n",
    "    Create a condensed matrix showing only significant daily correlations (lower triangle)\n",
    "    Now uses FDR-corrected p-values for significance\n",
    "    \"\"\"\n",
    "    # Filter out columns with all NaN or insufficient data\n",
    "    valid_columns = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dropna().shape[0] >= 3:\n",
    "            valid_columns.append(col)\n",
    "    \n",
    "    if len(valid_columns) < 2:\n",
    "        print(f\"Need at least 2 variables for correlation dataclouds on {subfolder}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate number of unique pairs\n",
    "    n_vars = len(valid_columns)\n",
    "    n_unique_pairs = n_vars * (n_vars - 1) // 2\n",
    "    \n",
    "    if n_unique_pairs == 0:\n",
    "        print(f\"Need at least 2 variables for correlation dataclouds on {subfolder}\")\n",
    "        return\n",
    "    \n",
    "    # Determine subplot layout\n",
    "    cols = min(4, n_unique_pairs)  # Max 4 columns\n",
    "    rows = (n_unique_pairs + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    suptitle = f'Daily Significant Correlations (FDR-corrected) - Dataclouds for {subfolder}'\n",
    "    fig.suptitle(suptitle, fontsize=14)\n",
    "    \n",
    "    # Make axes iterable\n",
    "    if n_unique_pairs == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if hasattr(axes, 'flatten') else axes\n",
    "    \n",
    "    plot_idx = 0\n",
    "    significant_pairs = []\n",
    "    \n",
    "    # Collect all pairs first (lower triangle)\n",
    "    for i, dim1 in enumerate(valid_columns):\n",
    "        for j, dim2 in enumerate(valid_columns):\n",
    "            if i <= j:  # Only lower triangle\n",
    "                continue\n",
    "                \n",
    "            # Get filtered data\n",
    "            filt_data = df[[dim1, dim2]].dropna()\n",
    "            \n",
    "            if len(filt_data) >= min_points and not np.isnan(kendall_mat.loc[dim1, dim2]):\n",
    "                corr_ken = kendall_mat.loc[dim1, dim2]\n",
    "                p_ken_raw = kendall_pval_mat.loc[dim1, dim2]\n",
    "                p_ken_fdr = kendall_pval_mat_fdr.loc[dim1, dim2]\n",
    "                n_points = int(kendall_len_mat.loc[dim1, dim2])\n",
    "                significant_pairs.append((dim1, dim2, filt_data, corr_ken, p_ken_raw, p_ken_fdr, n_points))\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    significant_pairs.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "    \n",
    "    # Plot significant pairs\n",
    "    for dim1, dim2, filt_data, corr_ken, p_ken_raw, p_ken_fdr, n_points in significant_pairs[:len(axes)]:\n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        x_data = filt_data[dim1]\n",
    "        y_data = filt_data[dim2]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        ax.scatter(x_data, y_data, alpha=0.6, s=30, color='steelblue')\n",
    "        \n",
    "        # Add trend line for FDR-significant correlations\n",
    "        if not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05:\n",
    "            z = np.polyfit(x_data, y_data, 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_trend = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "            ax.plot(x_trend, p(x_trend), \"r-\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Add statistics with FDR information\n",
    "        fdr_significance = \"*\" if (not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05) else \"\"\n",
    "        raw_significance = \"*\" if p_ken_raw < 0.05 else \"\"\n",
    "        \n",
    "        title_text = f'{dim1} vs {dim2}\\ntau={corr_ken:.3f}{fdr_significance}'\n",
    "        subtitle_text = f'p_raw={p_ken_raw:.3f}{raw_significance}'\n",
    "        if not np.isnan(p_ken_fdr):\n",
    "            subtitle_text += f', p_fdr={p_ken_fdr:.3f}{fdr_significance}'\n",
    "        subtitle_text += f', n={n_points}'\n",
    "        \n",
    "        ax.set_title(title_text, fontsize=10, fontweight='bold')\n",
    "        ax.text(0.5, -0.15, subtitle_text, transform=ax.transAxes, \n",
    "                ha='center', fontsize=8, style='italic')\n",
    "        \n",
    "        ax.set_xlabel(dim1, fontsize=9)\n",
    "        ax.set_ylabel(dim2, fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(plot_idx, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(mainfolder, subfolder, folder2, f\"filtered_daily_significant_dataclouds_fdr_{subfolder}_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "# Main per-day analysis loop with integrated dataclouds\n",
    "kendall_mat_dict = {}\n",
    "kendall_pval_mat_dict = {}\n",
    "kendall_pval_mat_fdr_dict = {}\n",
    "kendall_len_mat_dict = {}\n",
    "shapiro_pval_mat_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "for subfolder in norm_res.keys():\n",
    "    if subfolder == '25_3_24_n17_26_3_24_d': #instruction same as previous cell\n",
    "        continue\n",
    "    else:\n",
    "        dim_dict = {}\n",
    "        for dim_qx in norm_res[subfolder].keys():\n",
    "            if dim_qx in measures_to_pop:\n",
    "                continue\n",
    "                \n",
    "            dim_dict[dim_qx] = norm_res[subfolder][dim_qx]['full data']\n",
    "        df = pd.DataFrame(dim_dict) \n",
    "        #now need a corellogram such that for values =-5000 are turned to NaN and dropped only for the two columns being correlated. Kendall correlation using scipy needs to be done\n",
    "        df.replace(-5000, np.nan, inplace=True)\n",
    "        #shapiro_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "        shapiro_pval_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "        kendall_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "        kendall_pval_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "        kendall_len_mat = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "    \n",
    "        #for fdr correction\n",
    "        kendall_pval_mat_fdr = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)\n",
    "        \n",
    "        \n",
    "        for dim1 in df.columns:\n",
    "            for dim2 in df.columns:\n",
    "                filt_data = df[[dim1, dim2]] #.dropna() dropping nans after introducing lags\n",
    "                stat1, pval1, stat2, pval2, corr_ken, p_ken = lagged_kendall_tau(filt_data, lag)\n",
    "                \n",
    "    \n",
    "                if corr_ken!= np.nan:\n",
    "                    if pval1 > 0.05 and pval2 > 0.05:\n",
    "                        shapiro_pval_mat.loc[dim1, dim2] = 1\n",
    "                    else:\n",
    "                        shapiro_pval_mat.loc[dim1, dim2] = 0\n",
    "                    kendall_mat.loc[dim1, dim2] = corr_ken\n",
    "                    kendall_pval_mat.loc[dim1,dim2] = p_ken\n",
    "                    kendall_len_mat.loc[dim1,dim2] = len(filt_data)\n",
    "                else:\n",
    "                    #not enough data to test normality or calculate kendall corr\n",
    "                    shapiro_pval_mat.loc[dim1, dim2] = np.nan\n",
    "                    kendall_mat.loc[dim1, dim2] = np.nan\n",
    "                    kendall_pval_mat.loc[dim1,dim2] = np.nan\n",
    "                    kendall_len_mat.loc[dim1,dim2] = len(filt_data)\n",
    "    \n",
    "        #Applying fdr correction\n",
    "        temp_store = []\n",
    "        pPreFdr = []\n",
    "        pos = []\n",
    "        for meas_1 in df.columns:\n",
    "            temp_store.append(meas_1)\n",
    "            for meas_2 in df.columns:\n",
    "                if meas_1 == meas_2:\n",
    "                    continue\n",
    "                elif meas_2 in temp_store:\n",
    "                    continue\n",
    "                else:\n",
    "                    if not np.isnan(kendall_mat.loc[meas_1, meas_2]):\n",
    "                        pPreFdr.append(kendall_pval_mat.loc[meas_1, meas_2])\n",
    "                        pos.append((meas_1, meas_2))\n",
    "        \n",
    "        pPostFdr = multipletests(pPreFdr, method='fdr_bh')[1]\n",
    "        \n",
    "        for (meas_1, meas_2), p_adj in zip(pos, pPostFdr):\n",
    "            kendall_pval_mat_fdr.loc[meas_1, meas_2] = p_adj\n",
    "            kendall_pval_mat_fdr.loc[meas_2, meas_1] = p_adj\n",
    "        \n",
    "        #fill diagonal and any missing values with NaN \n",
    "        for meas in df.columns:\n",
    "            kendall_pval_mat_fdr.loc[meas, meas] = np.nan\n",
    "    \n",
    "        #storing values per day\n",
    "        kendall_mat_dict[subfolder] = kendall_mat.copy()\n",
    "        kendall_pval_mat_dict[subfolder] = kendall_pval_mat.copy()\n",
    "        kendall_pval_mat_fdr_dict[subfolder] = kendall_pval_mat_fdr.copy()\n",
    "        kendall_len_mat_dict[subfolder] = kendall_len_mat.copy()\n",
    "        shapiro_pval_mat_dict[subfolder] = shapiro_pval_mat.copy()\n",
    "    \n",
    "        #creating a custom annotation matrix combining tau, p-value, and data length\n",
    "        annotations = kendall_mat.copy()\n",
    "        \n",
    "        for dim1 in df.columns:\n",
    "            for dim2 in df.columns:\n",
    "                if not np.isnan(kendall_mat.loc[dim1, dim2]):\n",
    "                    annotations.loc[dim1, dim2] = f\"tau={kendall_mat.loc[dim1, dim2]:.2f}\\n\"\n",
    "                    if kendall_pval_mat.loc[dim1, dim2] < 0.05:\n",
    "                        annotations.loc[dim1, dim2] += f\"p={kendall_pval_mat.loc[dim1, dim2]:.2f}*\\n\"\n",
    "                    else:\n",
    "                        annotations.loc[dim1, dim2] += f\"p={kendall_pval_mat.loc[dim1, dim2]:.2f}\\n\"\n",
    "                    annotations.loc[dim1, dim2] += f\"dl={int(kendall_len_mat.loc[dim1, dim2])}\\n\"\n",
    "                    annotations.loc[dim1, dim2] += f\"norm={int(shapiro_pval_mat.loc[dim1, dim2])}\\n\"\n",
    "                    annotations.loc[dim1, dim2] += f\"lag_dim={dim1}\"\n",
    "                else:\n",
    "                    annotations.loc[dim1, dim2] = \"\"\n",
    "    \n",
    "        # Plot the combined heatmap with custom annotations (tau, p-value, and data length)\n",
    "        plt.figure(figsize=(32, 32))\n",
    "        sns.heatmap(kendall_mat, annot=annotations, fmt=\"\", cmap='coolwarm', square=True, cbar=True, mask=np.isnan(kendall_mat), annot_kws={\"size\": 12})\n",
    "        plt.title(f'Kendall Correlation, p-value, and Data Length for {subfolder}')\n",
    "        plt.savefig(os.path.join(mainfolder, subfolder, folder2, \"subjective_objective_corr_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        #repeating annotations and figure for fdr correction\n",
    "        #creating a custom annotation matrix combining normality test, tau, p-value, and data length\n",
    "        annotations_fdr = kendall_mat.copy()\n",
    "        for meas_1 in df.columns:\n",
    "            for meas_2 in df.columns:\n",
    "                if not np.isnan(kendall_mat.loc[meas_1, meas_2]):\n",
    "                    annotations_fdr.loc[meas_1, meas_2] = f\"tau={kendall_mat.loc[meas_1, meas_2]:.2f}\\n\" #tau\n",
    "                    if kendall_pval_mat_fdr.loc[meas_1, meas_2] < 0.05:\n",
    "                        annotations_fdr.loc[meas_1, meas_2] += f\"p={kendall_pval_mat_fdr.loc[meas_1, meas_2]:.2f}*\\n\"\n",
    "                    else:\n",
    "                        annotations_fdr.loc[meas_1, meas_2] += f\"p={kendall_pval_mat_fdr.loc[meas_1, meas_2]:.2f}\\n\"\n",
    "                    annotations_fdr.loc[meas_1, meas_2] += f\"dl={int(kendall_len_mat.loc[meas_1, meas_2])}\\n\"\n",
    "                    annotations_fdr.loc[meas_1, meas_2] += f\"norm={int(shapiro_pval_mat.loc[meas_1, meas_2])}\\n\"\n",
    "                    annotations_fdr.loc[meas_1, meas_2] += f\"lag_dim={meas_1}\"\n",
    "                else:\n",
    "                    annotations_fdr.loc[meas_1, meas_2] = \"\"\n",
    "        \n",
    "        # Plot the combined heatmap with custom annotations (tau, p-value, and data length)\n",
    "        plt.figure(figsize=(32, 32))\n",
    "        sns.heatmap(kendall_mat, annot=annotations_fdr, fmt=\"\", cmap='coolwarm', square=True, cbar=True, mask=np.isnan(kendall_mat), annot_kws={\"size\": 12})\n",
    "        title = 'Kendall Correlation, p-value (fdr corrected), and Data Length for day: '+ subfolder\n",
    "        plt.title(title)\n",
    "        plt.savefig(os.path.join(mainfolder, subfolder, folder2, \"subjective_objective_corr_fdr_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        # ========== ADD DATACLOUDS FOR EACH DAY ==========\n",
    "        print(f\"Creating daily datacloud matrix for {subfolder}...\")\n",
    "        create_daily_datacloud_matrix(df, subfolder, mainfolder,\n",
    "                                     kendall_mat, kendall_pval_mat, kendall_pval_mat_fdr, kendall_len_mat, lag, focused_folder)\n",
    "        \n",
    "        #print(f\"Creating condensed daily datacloud matrix for {subfolder}...\")\n",
    "        #create_daily_condensed_datacloud_matrix(df, subfolder, mainfolder, folder2,\n",
    "                                               #kendall_mat, kendall_pval_mat, kendall_pval_mat_fdr, kendall_len_mat)\n",
    "        \n",
    "        \n",
    "        print(f\"Completed datacloud analysis for {subfolder}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "#Same as previous cell\n",
    "\n",
    "partDates = list(norm_res.keys())  # Or norm_res.keys(), as appropriate\n",
    "#for asd_001: (modify as per participant and analysis needs)\n",
    "#partDates.remove('25_3_24_n17_26_3_24_d')\n",
    "\n",
    "#['dim_1_wakefullness', 'dim_2_boredom', 'dim_3_sensory_avoidance', 'dim_4_social avoidance', 'dim_5_physical tension', 'dim_6_scenario_anxiety', 'dim_7_rumination', 'dim_8_stress', 'dim_9_pain', 'eda', 'pulse_rate', 'pulse_rate_variability', 'resp_rate', 'temp', 'steps', 'acc_std_dev', 'activity', 'met', 'wearing_det']\n",
    "#(modify as per participant and analysis needs)\n",
    "meas_list = ['dim_1_wakefullness', 'dim_5_physical tension', 'dim_8_stress', 'eda'] \n",
    "measure_pairs = [('dim_8_stress', meas) for meas in meas_list if meas != 'dim_8_stress']\n",
    "measure_pairs += [('eda', meas) for meas in meas_list if meas != 'eda']\n",
    "measure_pairs += [('dim_8_stress', 'eda')]\n",
    "measure_pairs += [('dim_1_wakefullness', meas) for meas in meas_list if meas != 'dim_1_wakefullness']\n",
    "measure_pairs += [('dim_5_physical tension', meas) for meas in meas_list if meas != 'dim_5_physical tension']\n",
    "\n",
    "for var1, var2 in measure_pairs:\n",
    "    tau_vals = []\n",
    "    p_raw_vals = []\n",
    "    p_fdr_vals = []\n",
    "    annotation_texts = []\n",
    "    tick_labels = []\n",
    "    valid_day_indices = []\n",
    "\n",
    "    for idx, day_key in enumerate(partDates):\n",
    "        try:\n",
    "            parts = day_key.split('_')\n",
    "            reqYear = 2000 + int(parts[-2])  # adjust if your data uses 1900-base years - unlikely\n",
    "            reqMonth = int(parts[-3])\n",
    "            reqDay = int(parts[-4])\n",
    "            this_label = giv_date_and_day(reqYear, reqMonth, reqDay)\n",
    "        except Exception:\n",
    "            this_label = day_key\n",
    "\n",
    "        # Retrieve correlation values\n",
    "        kendall = kendall_mat_dict[day_key]\n",
    "        pval = kendall_pval_mat_dict[day_key]\n",
    "        pval_fdr = kendall_pval_mat_fdr_dict[day_key]\n",
    "        if var1 in kendall.index and var2 in kendall.columns and not np.isnan(kendall.loc[var1, var2]):\n",
    "            tau = kendall.loc[var1, var2]\n",
    "            p = pval.loc[var1, var2]\n",
    "            p_fdr = pval_fdr.loc[var1, var2]\n",
    "            tau_vals.append(tau)\n",
    "            p_raw_vals.append(p)\n",
    "            p_fdr_vals.append(p_fdr)\n",
    "            if p_fdr < 0.05:\n",
    "                annotation_texts.append(f\"tau={tau:.2f}*\\np={p:.2g}\\np_fdr={p_fdr:.2g}\")\n",
    "            else:\n",
    "                annotation_texts.append(f\"tau={tau:.2f}\\np={p:.2g}\\np_fdr={p_fdr:.2g}\")\n",
    "            tick_labels.append(this_label)\n",
    "            valid_day_indices.append(idx)\n",
    "\n",
    "    # Now plot only days with valid data\n",
    "    fig, ax = plt.subplots(figsize=(max(10, len(valid_day_indices)*0.7), 6))\n",
    "    markerline, stemlines, baseline = ax.stem(range(len(tau_vals)), tau_vals, linefmt='g-', markerfmt='go', basefmt='r-')\n",
    "    ax.axhline(0, color='red', linestyle='dashed')\n",
    "    plt.xticks(range(len(tau_vals)), tick_labels, rotation=45, fontsize=10)\n",
    "    ax.set_ylabel(\"Correlation (Kendall's tau)\")\n",
    "    ax.set_xlabel(\"Day (date + weekday)\")\n",
    "    #for graph naming\n",
    "    var1 = var1.split('_')[-1]\n",
    "    var2 = var2.split('_')[-1]\n",
    "    \n",
    "    ax.set_title(f\"Per-day Correlation: {var1} (lagged) vs {var2}\")\n",
    "\n",
    "    # Annotate at the top of each stick\n",
    "    for i, txt in enumerate(annotation_texts):\n",
    "        ax.text(i, tau_vals[i], txt, ha='left', va='bottom', fontsize=5, color='brown')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(focused_folder, f\"{var1}_{var2}_corr_fdr_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b48bd-6bd8-4184-8112-14218091c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW FOR CORRELATIONS OF ALL DAYS TOGETHER TO GET A SENSE OF OVERALL TRENDS BETWEEN MEASURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892c075-835b-42d3-b258-7604f7b61984",
   "metadata": {},
   "outputs": [],
   "source": [
    "sublist = list(dim_q.keys())\n",
    "sublist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456070d-4193-4520-a4e1-7a51a97c487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oblist = list(dict_meas.keys())\n",
    "oblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4006e-06c8-467a-92e2-504ac59739f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subOb = sublist+oblist\n",
    "subOb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198e1f7-aaaa-44f8-af72-77480fb31cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subOb_dict = dim_q | dict_meas\n",
    "subOb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e2255-ec9d-4bcd-99c0-38adc116ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subOb_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685ddde-0625-4dfa-9c45-dd094846185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ALL DAY CORELLOGRAM\n",
    "\"\"\"\n",
    "\n",
    "kendall_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "kendall_mat_all_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5eff85-fefb-4c90-9bc0-626b5cbcbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL: If not all measures need to be analysed, list the the ones to exclude below (example shown). If not, skip cell\n",
    "measures_to_pop = ['dim_2_boredom', 'dim_3_sensory_avoidance', 'dim_4_social avoidance', 'dim_6_scenario_anxiety', 'temp', 'steps', 'activity', 'met', 'wearing_det'] \n",
    "\n",
    "subOb = [x for x in subOb if x not in measures_to_pop]\n",
    "\n",
    "for key in measures_to_pop:\n",
    "    subOb_dict.pop(key, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fd1c1-74f1-4d49-aa6d-6f44856e2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL: If some days have to be excluded, list them below and execute cell. If not, skip\n",
    "days_to_pop = ['08_3_24_n1_9_3_24_d', '11_3_24_n3_12_3_24_d', '12_3_24_n4_13_3_24_d', '23_3_24_n15_24_3_24_d', '24_3_24_n16_25_3_24_d', '25_3_24_n17_26_3_24_d']\n",
    "\"\"\"\n",
    "for meas in subOb_dict.keys():\n",
    "    #print(day)\n",
    "    for day in subOb_dict[meas].keys():\n",
    "        if day in days_to_pop:\n",
    "            subOb_dict[meas].pop(day, None)\n",
    "\"\"\"\n",
    "for meas in subOb_dict.keys():\n",
    "    for day in list(subOb_dict[meas].keys()):  # make a list copy\n",
    "        if day in days_to_pop:\n",
    "            subOb_dict[meas].pop(day, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7b3cf-c289-4d00-8514-88e43865c832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "JUST LIKE THE PER-DAY VERSION IN PREVIOUS CELLS\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "shapiro_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "shapiro_pval_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "#for correlation test\n",
    "kendall_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "kendall_pval_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "kendall_len_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "#for fdr correction\n",
    "kendall_pval_mat_all_day_fdr = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "\n",
    "\n",
    "for dim_qx in subOb_dict:\n",
    "    for dim_qy in subOb_dict:\n",
    "        dimqx = []\n",
    "        dimqy = []\n",
    "        for subfolder in subOb_dict[dim_qx].keys():\n",
    "            if subfolder in subOb_dict[dim_qy].keys():\n",
    "                #print(subfolder)\n",
    "                #print(f'Appending data for common subfolder: {subfolder} for dims {dim_qx} and {dim_qy}')\n",
    "                for val_x in subOb_dict[dim_qx][subfolder].values():\n",
    "                    dimqx.append(val_x)\n",
    "                for val_y in subOb_dict[dim_qy][subfolder].values():\n",
    "                    dimqy.append(val_y)\n",
    "\n",
    "        df = pd.DataFrame({f'{dim_qx}_1' : dimqx, f'{dim_qy}_2' : dimqy}) \n",
    "        #filter the data by turning -5000 values to nan and then dropping the nans\n",
    "        df.replace(-5000, np.nan, inplace=True)\n",
    "        filt_data = df.dropna()\n",
    "        if len(filt_data)>=2:\n",
    "                stat1, pval1 = shapiro(filt_data[f'{dim_qx}_1'])\n",
    "                stat2, pval2 = shapiro(filt_data[f'{dim_qy}_2'])\n",
    "                corr_ken, p_ken = kendalltau(filt_data[f'{dim_qx}_1'], filt_data[f'{dim_qy}_2'])\n",
    "                if pval1 > 0.05 and pval2 > 0.05:\n",
    "                    shapiro_pval_mat_all_day.loc[dim_qx, dim_qy] = 1\n",
    "                else:\n",
    "                    shapiro_pval_mat_all_day.loc[dim_qx, dim_qy] = 0\n",
    "                kendall_mat_all_day.loc[dim_qx, dim_qy] = corr_ken\n",
    "                kendall_pval_mat_all_day.loc[dim_qx, dim_qy] = p_ken\n",
    "                kendall_len_mat_all_day.loc[dim_qx, dim_qy] = len(filt_data)\n",
    "        else:\n",
    "                #not enough data to test normality or calculate kendall corr\n",
    "                shapiro_pval_mat_all_day.loc[dim_qx, dim_qy] = np.nan\n",
    "                kendall_mat_all_day.loc[dim_qx, dim_qy] = np.nan\n",
    "                kendall_pval_mat_all_day.loc[dim_qx, dim_qy] = np.nan\n",
    "                kendall_len_mat_all_day.loc[dim_qx, dim_qy] = len(filt_data)\n",
    "\n",
    "#Applying fdr correction\n",
    "#\n",
    "temp_store = []\n",
    "pPreFdr = []\n",
    "pos = []\n",
    "for meas_1 in subOb_dict:\n",
    "    temp_store.append(meas_1)\n",
    "    for meas_2 in subOb_dict:\n",
    "        if meas_1 == meas_2:\n",
    "            continue\n",
    "        elif meas_2 in temp_store:\n",
    "            continue\n",
    "        else:\n",
    "            if not np.isnan(kendall_mat_all_day.loc[meas_1, meas_2]):\n",
    "                pPreFdr.append(kendall_pval_mat_all_day.loc[meas_1, meas_2])\n",
    "                pos.append((meas_1, meas_2))\n",
    "\n",
    "pPostFdr = multipletests(pPreFdr, method='fdr_bh')[1]\n",
    "\n",
    "for (meas_1, meas_2), p_adj in zip(pos, pPostFdr):\n",
    "    kendall_pval_mat_all_day_fdr.loc[meas_1, meas_2] = p_adj\n",
    "    kendall_pval_mat_all_day_fdr.loc[meas_2, meas_1] = p_adj\n",
    "\n",
    "#fill diagonal and any missing values with NaN \n",
    "for meas in subOb_dict:\n",
    "    kendall_pval_mat_all_day_fdr.loc[meas, meas] = np.nan\n",
    "\n",
    "#creating a custom annotation matrix combining normality test, tau, p-value, and data length\n",
    "annotations = kendall_mat_all_day.copy()\n",
    "for dim_qx in subOb_dict:\n",
    "    for dim_qy in subOb_dict:\n",
    "        if not np.isnan(kendall_mat_all_day.loc[dim_qx, dim_qy]):\n",
    "            annotations.loc[dim_qx, dim_qy] = f\"tau={kendall_mat_all_day.loc[dim_qx, dim_qy]:.2f}\\n\"\n",
    "            if kendall_pval_mat_all_day.loc[dim_qx, dim_qy] < 0.05:\n",
    "                annotations.loc[dim_qx, dim_qy] += f\"p={kendall_pval_mat_all_day.loc[dim_qx, dim_qy]:.3f}*\\n\"\n",
    "            else:\n",
    "                annotations.loc[dim_qx, dim_qy] += f\"p={kendall_pval_mat_all_day.loc[dim_qx, dim_qy]:.3f}\\n\"\n",
    "            annotations.loc[dim_qx, dim_qy] += f\"dl={int(kendall_len_mat_all_day.loc[dim_qx, dim_qy])}\\n\"\n",
    "            annotations.loc[dim_qx, dim_qy] += f\"norm={int(shapiro_pval_mat_all_day.loc[dim_qx, dim_qy])}\"\n",
    "        else:\n",
    "            annotations.loc[dim_qx, dim_qy] = \"\"\n",
    "\n",
    "# Plot the combined heatmap with custom annotations (tau, p-value, and data length)\n",
    "plt.figure(figsize=(32, 32))\n",
    "sns.heatmap(kendall_mat_all_day, annot=annotations, fmt=\"\", cmap='coolwarm', square=True, cbar=True, mask=np.isnan(kendall_mat_all_day), annot_kws={\"size\": 12})\n",
    "title = 'Kendall Correlation, p-value, and Data Length for participant: '+ mainfolder.split(\"\\\\\")[-1]\n",
    "plt.title(title)\n",
    "plt.savefig(os.path.join(mainfolder, \"all_day_subjective_objective_correlogram_fifteen_min_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#repeating annotations and figure for fdr correction\n",
    "annotations_fdr = kendall_mat_all_day.copy()\n",
    "for meas_1 in subOb_dict:\n",
    "    for meas_2 in subOb_dict:\n",
    "        if not np.isnan(kendall_mat_all_day.loc[meas_1, meas_2]):\n",
    "            annotations_fdr.loc[meas_1, meas_2] = f\"tau={kendall_mat_all_day.loc[meas_1, meas_2]:.2f}\\n\"\n",
    "            if kendall_pval_mat_all_day_fdr.loc[meas_1, meas_2] < 0.05:\n",
    "                annotations_fdr.loc[meas_1, meas_2] += f\"p={kendall_pval_mat_all_day_fdr.loc[meas_1, meas_2]:.2f}*\\n\"\n",
    "            else:\n",
    "                annotations_fdr.loc[meas_1, meas_2] += f\"p={kendall_pval_mat_all_day_fdr.loc[meas_1, meas_2]:.2f}\\n\"\n",
    "            annotations_fdr.loc[meas_1, meas_2] += f\"dl={int(kendall_len_mat_all_day.loc[meas_1, meas_2])}\\n\"\n",
    "            annotations_fdr.loc[meas_1, meas_2] += f\"norm={int(shapiro_pval_mat_all_day.loc[meas_1, meas_2])}\"\n",
    "        else:\n",
    "            annotations_fdr.loc[meas_1, meas_2] = \"\"\n",
    "\n",
    "# Plot the combined heatmap with custom annotations (tau, p-value, and data length)\n",
    "plt.figure(figsize=(32, 32))\n",
    "sns.heatmap(kendall_mat_all_day, annot=annotations_fdr, fmt=\"\", cmap='coolwarm', square=True, cbar=True, mask=np.isnan(kendall_mat_all_day), annot_kws={\"size\": 12})\n",
    "title = 'Kendall Correlation, p-value (fdr corrected), and Data Length for participant: '+ mainfolder.split(\"\\\\\")[-1]\n",
    "plt.title(title)\n",
    "plt.savefig(os.path.join(mainfolder, \"all_day_subjective_objective_correlogram_fifteen_min_fdr_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#trying out dataclouds with FDR correction\n",
    "\n",
    "def create_datacloud_matrix(subOb_dict, subOb, mainfolder, kendall_mat_all_day, kendall_pval_mat_all_day, kendall_pval_mat_all_day_fdr, kendall_len_mat_all_day, min_points=10):\n",
    "    \"\"\"\n",
    "    Create a matrix of scatter plots (dataclouds) corresponding to correlation analysis\n",
    "    Now uses FDR-corrected p-values for significance\n",
    "    \"\"\"\n",
    "    special_var = {'eda': 30, 'pulse_rate':150, 'pulse_rate_variability':300, 'resp_rate':30, 'acc_std_dev':0.5}  #specify limits as required\n",
    "    n_vars = len(subOb)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_vars, n_vars, figsize=(3*n_vars, 2*n_vars))\n",
    "    participant_title_list = mainfolder.split(\"\\\\\")[-1].split('_')[0:3]\n",
    "    participant_title = '_'.join(participant_title_list)\n",
    "    suptitle = 'Datacloud Matrix (FDR-corrected) for participant: '+ participant_title\n",
    "    fig.suptitle(suptitle, fontsize=26, y=0.98)\n",
    "    \n",
    "    # If only one variable, make axes 2D for consistent indexing\n",
    "    if n_vars == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_vars == 2:\n",
    "        axes = axes.reshape(2, 2)\n",
    "    \n",
    "    for i, dim_qx in enumerate(subOb):\n",
    "        for j, dim_qy in enumerate(subOb):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Collect data for this pair\n",
    "            dimqx = []\n",
    "            dimqy = []\n",
    "            \n",
    "            for subfolder in subOb_dict[dim_qx].keys():\n",
    "                if subfolder in subOb_dict[dim_qy].keys():\n",
    "                    for val_x in subOb_dict[dim_qx][subfolder].values():\n",
    "                        dimqx.append(val_x)\n",
    "                    for val_y in subOb_dict[dim_qy][subfolder].values():\n",
    "                        dimqy.append(val_y)\n",
    "            \n",
    "            # Create DataFrame and filter data\n",
    "            df = pd.DataFrame({f'{dim_qx}_1': dimqx, f'{dim_qy}_2': dimqy})\n",
    "            df.replace(-5000, np.nan, inplace=True)\n",
    "            filt_data = df.dropna()\n",
    "            \n",
    "            if len(filt_data) >= 2:\n",
    "                x_data = filt_data[f'{dim_qx}_1']\n",
    "                y_data = filt_data[f'{dim_qy}_2']\n",
    "                \n",
    "                if i == j:  # Diagonal - leave blank\n",
    "                    ax.text(0.5, 0.5, f'Diagonal', \n",
    "                           transform=ax.transAxes, ha='center', va='center',\n",
    "                           fontsize=10, color='red')\n",
    "                    ax.set_xlabel(dim_qx, fontsize=9)\n",
    "                    ax.set_ylabel(dim_qy, fontsize=9)\n",
    "                elif i>j:  # Off-diagonal - show scatter plot\n",
    "                    # Create scatter plot\n",
    "                    ax.scatter(x_data, y_data, alpha=0.6, s=20, color='steelblue')\n",
    "                    if dim_qx in special_var.keys():\n",
    "                        for key, val in special_var.items():\n",
    "                            if dim_qx == key:\n",
    "                                ax.set_xlim(0, val)\n",
    "                    else:\n",
    "                        ax.set_xlim(0, 4)\n",
    "                    if dim_qy in special_var.keys():\n",
    "                        for key, val in special_var.items():\n",
    "                            if dim_qy == key:\n",
    "                                ax.set_ylim(0, val)\n",
    "                    else:\n",
    "                        ax.set_ylim(0, 4)\n",
    "                    \n",
    "                    # Get correlation statistics from existing matrices\n",
    "                    if len(filt_data) >= min_points and not np.isnan(kendall_mat_all_day.loc[dim_qx, dim_qy]):\n",
    "                        corr_ken = kendall_mat_all_day.loc[dim_qx, dim_qy]\n",
    "                        p_ken_raw = kendall_pval_mat_all_day.loc[dim_qx, dim_qy]\n",
    "                        p_ken_fdr = kendall_pval_mat_all_day_fdr.loc[dim_qx, dim_qy]\n",
    "                        n_points = int(kendall_len_mat_all_day.loc[dim_qx, dim_qy])\n",
    "                        \n",
    "                        # Add trend line if FDR-corrected correlation is significant and reasonably strong\n",
    "                        if not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05 : #and abs(corr_ken) > 0.2\n",
    "                            # Fit linear trend line\n",
    "                            z = np.polyfit(x_data, y_data, 1)\n",
    "                            p = np.poly1d(z)\n",
    "                            ax.plot(x_data.sort_values(), p(x_data.sort_values()), \n",
    "                                   \"r--\", alpha=0.8, linewidth=1.5)\n",
    "                        \n",
    "                        # Add correlation info with both raw and FDR p-values\n",
    "                        fdr_sig = \"*\" if (not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05) else \"\"\n",
    "                        raw_sig = \"*\" if p_ken_raw < 0.05 else \"\"\n",
    "                        \n",
    "                        annotation_text = f'tau={corr_ken:.3f}{fdr_sig}\\n'\n",
    "                        annotation_text += f'p_raw={p_ken_raw:.3f}{raw_sig}\\n'\n",
    "                        if not np.isnan(p_ken_fdr):\n",
    "                            annotation_text += f'p_fdr={p_ken_fdr:.3f}{fdr_sig}\\n'\n",
    "                        else:\n",
    "                            annotation_text += f'p_fdr=n/a\\n'\n",
    "                        annotation_text += f'n={n_points}'\n",
    "                        \n",
    "                        ax.text(0.05, 0.95, annotation_text,\n",
    "                               transform=ax.transAxes, fontsize=7,\n",
    "                               verticalalignment='top',\n",
    "                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                    \n",
    "                    ax.set_xlabel(dim_qx, fontsize=9)\n",
    "                    ax.set_ylabel(dim_qy, fontsize=9)\n",
    "                else:\n",
    "                    #upper triangle\n",
    "                    ax.text(0.5, 0.5, f'Upper triangle', \n",
    "                           transform=ax.transAxes, ha='center', va='center',\n",
    "                           fontsize=10, color='red')\n",
    "                    ax.set_xlabel(dim_qx, fontsize=9)\n",
    "                    ax.set_ylabel(dim_qy, fontsize=9)\n",
    "                    if dim_qx == special_var:\n",
    "                        ax.set_xlim(0, 30)\n",
    "                    else:\n",
    "                        ax.set_xlim(0, 4)\n",
    "                    if dim_qy == special_var:\n",
    "                        ax.set_ylim(0, 30)\n",
    "                    else:\n",
    "                        ax.set_ylim(0, 4)\n",
    "            else:\n",
    "                # Not enough data\n",
    "                ax.text(0.5, 0.5, f'Insufficient data\\n(n={len(filt_data)})', \n",
    "                       transform=ax.transAxes, ha='center', va='center',\n",
    "                       fontsize=10, color='red')\n",
    "                ax.set_xlabel(dim_qx, fontsize=9)\n",
    "                ax.set_ylabel(dim_qy, fontsize=9)\n",
    "                if dim_qx == special_var:\n",
    "                    ax.set_xlim(0, 30)\n",
    "                else:\n",
    "                    ax.set_xlim(0, 4)\n",
    "                if dim_qy == special_var:\n",
    "                    ax.set_ylim(0, 30)\n",
    "                else:\n",
    "                    ax.set_ylim(0, 4)\n",
    "            \n",
    "            # Clean up axes\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    focus_folder = r'dataclouds_CAM_LMU\\Stream_ASD_001\\all_days' #specify as required or also change it to mainfolder\n",
    "    plt.savefig(os.path.join(focus_folder, \"focused_datacloud_matrix_fdr_fifteen_min_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_condensed_datacloud_matrix(subOb_dict, subOb, mainfolder, kendall_mat_all_day, kendall_pval_mat_all_day, kendall_pval_mat_all_day_fdr, kendall_len_mat_all_day, min_points=10):\n",
    "    \"\"\"\n",
    "    In descending order correlation strength\n",
    "    Create a condensed matrix showing only unique pairs (lower triangle)\n",
    "    Now uses FDR-corrected p-values for significance\n",
    "    \"\"\"\n",
    "    # Calculate number of unique pairs\n",
    "    n_vars = len(subOb)\n",
    "    n_unique_pairs = n_vars * (n_vars - 1) // 2\n",
    "    \n",
    "    if n_unique_pairs == 0:\n",
    "        print(\"Need at least 2 variables for correlation dataclouds\")\n",
    "        return\n",
    "    \n",
    "    # Determine subplot layout\n",
    "    cols = min(5, n_unique_pairs)  # Max 5 columns\n",
    "    rows = (n_unique_pairs + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    suptitle = 'Significant Correlations (FDR-corrected) - Dataclouds for participant: '+mainfolder.split(\"\\\\\")[-1]\n",
    "    fig.suptitle(suptitle, fontsize=14)\n",
    "    \n",
    "    # Make axes iterable\n",
    "    if n_unique_pairs == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if hasattr(axes, 'flatten') else axes\n",
    "    \n",
    "    plot_idx = 0\n",
    "    significant_pairs = []\n",
    "    \n",
    "    # Collect all pairs first (lower triangle)\n",
    "    for i, dim_qx in enumerate(subOb):\n",
    "        for j, dim_qy in enumerate(subOb):\n",
    "            if i <= j:  # Only lower triangle\n",
    "                continue\n",
    "                \n",
    "            # Collect and filter data\n",
    "            dimqx = []\n",
    "            dimqy = []\n",
    "            \n",
    "            for subfolder in subOb_dict[dim_qx].keys():\n",
    "                if subfolder in subOb_dict[dim_qy].keys():\n",
    "                    for val_x in subOb_dict[dim_qx][subfolder].values():\n",
    "                        dimqx.append(val_x)\n",
    "                    for val_y in subOb_dict[dim_qy][subfolder].values():\n",
    "                        dimqy.append(val_y)\n",
    "            \n",
    "            df = pd.DataFrame({f'{dim_qx}_1': dimqx, f'{dim_qy}_2': dimqy})\n",
    "            df.replace(-5000, np.nan, inplace=True)\n",
    "            filt_data = df.dropna()\n",
    "            \n",
    "            if len(filt_data) >= min_points and not np.isnan(kendall_mat_all_day.loc[dim_qx, dim_qy]):\n",
    "                corr_ken = kendall_mat_all_day.loc[dim_qx, dim_qy]\n",
    "                p_ken_raw = kendall_pval_mat_all_day.loc[dim_qx, dim_qy]\n",
    "                p_ken_fdr = kendall_pval_mat_all_day_fdr.loc[dim_qx, dim_qy]\n",
    "                n_points = int(kendall_len_mat_all_day.loc[dim_qx, dim_qy])\n",
    "                significant_pairs.append((dim_qx, dim_qy, filt_data, corr_ken, p_ken_raw, p_ken_fdr, n_points))\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    significant_pairs.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "    \n",
    "    # Plot significant pairs\n",
    "    for dim_qx, dim_qy, filt_data, corr_ken, p_ken_raw, p_ken_fdr, n_points in significant_pairs[:len(axes)]:\n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        x_data = filt_data[f'{dim_qx}_1']\n",
    "        y_data = filt_data[f'{dim_qy}_2']\n",
    "        \n",
    "        # Create scatter plot\n",
    "        ax.scatter(x_data, y_data, alpha=0.6, s=30, color='steelblue')\n",
    "        \n",
    "        # Add trend line for FDR-significant correlations\n",
    "        if not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05:\n",
    "            z = np.polyfit(x_data, y_data, 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_trend = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "            ax.plot(x_trend, p(x_trend), \"r-\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Add statistics with FDR information\n",
    "        fdr_significance = \"*\" if (not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05) else \"\"\n",
    "        raw_significance = \"*\" if p_ken_raw < 0.05 else \"\"\n",
    "        \n",
    "        title_text = f'{dim_qx} vs {dim_qy}\\ntau={corr_ken:.3f}{fdr_significance}'\n",
    "        subtitle_text = f'p_raw={p_ken_raw:.3f}{raw_significance}'\n",
    "        if not np.isnan(p_ken_fdr):\n",
    "            subtitle_text += f', p_fdr={p_ken_fdr:.3f}{fdr_significance}'\n",
    "        subtitle_text += f', n={n_points}'\n",
    "        \n",
    "        ax.set_title(title_text, fontsize=10, fontweight='bold')\n",
    "        ax.text(0.5, -0.15, subtitle_text, transform=ax.transAxes, \n",
    "                ha='center', fontsize=8, style='italic')\n",
    "        \n",
    "        ax.set_xlabel(dim_qx, fontsize=9)\n",
    "        ax.set_ylabel(dim_qy, fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(plot_idx, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(mainfolder, \"filtered_significant_dataclouds_fdr_fifteen_min_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Updated function calls with FDR correction:\n",
    "\n",
    "print(\"Creating full datacloud matrix with FDR correction...\")\n",
    "create_datacloud_matrix(subOb_dict, list(subOb_dict.keys()), mainfolder, \n",
    "                        kendall_mat_all_day, kendall_pval_mat_all_day, \n",
    "                        kendall_pval_mat_all_day_fdr, kendall_len_mat_all_day)\n",
    "\n",
    "#print(\"Creating condensed datacloud matrix with FDR correction...\")\n",
    "#create_condensed_datacloud_matrix(subOb_dict, list(subOb_dict.keys()), mainfolder,\n",
    "                                 #kendall_mat_all_day, kendall_pval_mat_all_day,\n",
    "                                 #kendall_pval_mat_all_day_fdr, kendall_len_mat_all_day)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a24e55-c85a-49f8-b9a5-f8f6a7722f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SAME AS ABOVE BUT WITH LAG \n",
    "\"\"\"\n",
    "\n",
    "lag = 32\n",
    "\n",
    "focus_folder = r'dataclouds_CAM_LMU\\Stream_ASD_001\\all_days'\n",
    "folder_req = os.path.join(focus_folder, 'lag', str(lag))\n",
    "\n",
    "\n",
    "if not os.path.exists(folder_req):\n",
    "    os.makedirs(folder_req, exist_ok=True)\n",
    "focused_folder = folder_req\n",
    "\n",
    "\n",
    "def giv_date_and_day(reqYear, reqMonth, reqDay):\n",
    "    date_obj = datetime(reqYear, reqMonth, reqDay)\n",
    "    return date_obj.strftime(\"%Y-%m-%d\\n%a\")  \n",
    "\n",
    "def lagged_kendall_tau(df, lag):\n",
    "    # Shift one column by lag (forward lag; negative for backward lag)\n",
    "    shifted = df.copy()\n",
    "    col1 = df.columns[0]\n",
    "    col2 = df.columns[1]\n",
    "    shifted[col1] = shifted[col1].shift(lag)\n",
    "    # Drop rows with any NaN (either initial or from shift)\n",
    "    clean = shifted.dropna(subset=[col1, col2])\n",
    "    nData = len(clean)\n",
    "    \n",
    "    stat1, pval1, stat2, pval2, tau, p_value = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    if len(clean)>=3:\n",
    "        stat1, pval1 = shapiro(clean[col1])\n",
    "        stat2, pval2 = shapiro(clean[col2])\n",
    "        # Compute Kendall's tau\n",
    "        tau, p_value = kendalltau(clean[col1], clean[col2])\n",
    "    return stat1, pval1, stat2, pval2, tau, p_value, nData\n",
    "\n",
    "def lagged_per_day_vals(df, lag):\n",
    "    # Shift one column by lag (forward lag; negative for backward lag)\n",
    "    shifted = df.copy()\n",
    "    shifted.replace(-5000, np.nan, inplace=True)\n",
    "    col1 = df.columns[0]\n",
    "    col2 = df.columns[1]\n",
    "    shifted[col1] = shifted[col1].shift(lag)\n",
    "    # Drop rows with any NaN (either initial or from shift)\n",
    "    clean = shifted.dropna(subset=[col1, col2])\n",
    "    return clean\n",
    "\n",
    "\n",
    "shapiro_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "shapiro_pval_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "#for correlation test\n",
    "kendall_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "kendall_pval_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "kendall_len_mat_all_day = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "#for fdr correction\n",
    "kendall_pval_mat_all_day_fdr = pd.DataFrame(np.zeros((len(subOb), len(subOb))), columns=subOb, index=subOb)\n",
    "  \n",
    "for dim_qx in subOb_dict:\n",
    "    for dim_qy in subOb_dict:\n",
    "        #print(dim_qx , dim_qy)\n",
    "        dimqx = []\n",
    "        dimqy = []\n",
    "        for subfolder in subOb_dict[dim_qx].keys():\n",
    "            if subfolder in subOb_dict[dim_qy].keys():\n",
    "                #print(subfolder)\n",
    "                #print(f'Appending data for common subfolder: {subfolder} for dims {dim_qx} and {dim_qy}')\n",
    "                #creating a temporary store of per day values. The temp_dimqx will be shifted by the required lag and then both lists will be appended to dimqx and dimqy\n",
    "                temp_dimqx = []\n",
    "                temp_dimqy = []\n",
    "                for val_x in subOb_dict[dim_qx][subfolder].values():\n",
    "                    temp_dimqx.append(val_x)\n",
    "                for val_y in subOb_dict[dim_qy][subfolder].values():\n",
    "                    temp_dimqy.append(val_y)\n",
    "                temp_df = pd.DataFrame({f'{dim_qx}_1': temp_dimqx, f'{dim_qy}_2': temp_dimqy})\n",
    "                clean_temp_df = lagged_per_day_vals(temp_df, lag)\n",
    "                for val_x in clean_temp_df[f'{dim_qx}_1']:\n",
    "                    #print('appending shifted values of this day of the dimension: ', f'{dim_qx}')\n",
    "                    dimqx.append(val_x)\n",
    "                for val_y in clean_temp_df[f'{dim_qy}_2']:\n",
    "                    #print('appending non-shifted values of this day of the dimension: ', f'{dim_qy}')\n",
    "                    dimqy.append(val_y)\n",
    "                \n",
    "\n",
    "        filt_data = pd.DataFrame({f'{dim_qx}_1' : dimqx, f'{dim_qy}_2' : dimqy}) \n",
    "        \n",
    "        \n",
    "        if len(filt_data)>=2:\n",
    "                stat1, pval1 = shapiro(filt_data[f'{dim_qx}_1'])\n",
    "                stat2, pval2 = shapiro(filt_data[f'{dim_qy}_2'])\n",
    "                corr_ken, p_ken = kendalltau(filt_data[f'{dim_qx}_1'], filt_data[f'{dim_qy}_2'])\n",
    "                if pval1 > 0.05 and pval2 > 0.05:\n",
    "                    shapiro_pval_mat_all_day.loc[dim_qx, dim_qy] = 1\n",
    "                else:\n",
    "                    shapiro_pval_mat_all_day.loc[dim_qx, dim_qy] = 0\n",
    "                kendall_mat_all_day.loc[dim_qx, dim_qy] = corr_ken\n",
    "                kendall_pval_mat_all_day.loc[dim_qx, dim_qy] = p_ken\n",
    "                kendall_len_mat_all_day.loc[dim_qx, dim_qy] = len(filt_data)\n",
    "        else:\n",
    "                #not enough data to test normality or calculate kendall corr\n",
    "                shapiro_pval_mat_all_day.loc[dim_qx, dim_qy] = np.nan\n",
    "                kendall_mat_all_day.loc[dim_qx, dim_qy] = np.nan\n",
    "                kendall_pval_mat_all_day.loc[dim_qx, dim_qy] = np.nan\n",
    "                kendall_len_mat_all_day.loc[dim_qx, dim_qy] = len(filt_data)\n",
    "\n",
    "#Applying fdr correction\n",
    "#\n",
    "temp_store = []\n",
    "pPreFdr = []\n",
    "pos = []\n",
    "for meas_1 in subOb_dict:\n",
    "    temp_store.append(meas_1)\n",
    "    for meas_2 in subOb_dict:\n",
    "        #print(meas_1, meas_2)\n",
    "        \n",
    "        if not np.isnan(kendall_mat_all_day.loc[meas_1, meas_2]):\n",
    "                pPreFdr.append(kendall_pval_mat_all_day.loc[meas_1, meas_2])\n",
    "                pos.append((meas_1, meas_2))\n",
    "\n",
    "pPostFdr = multipletests(pPreFdr, method='fdr_bh')[1]\n",
    "\n",
    "for (meas_1, meas_2), p_adj in zip(pos, pPostFdr):\n",
    "    kendall_pval_mat_all_day_fdr.loc[meas_1, meas_2] = p_adj\n",
    "    kendall_pval_mat_all_day_fdr.loc[meas_2, meas_1] = p_adj\n",
    "\n",
    "\n",
    "#creating a custom annotation matrix combining normality test, tau, p-value, and data length\n",
    "annotations = kendall_mat_all_day.copy()\n",
    "for dim_qx in subOb_dict:\n",
    "    for dim_qy in subOb_dict:\n",
    "        if not np.isnan(kendall_mat_all_day.loc[dim_qx, dim_qy]):\n",
    "            annotations.loc[dim_qx, dim_qy] = f\"tau={kendall_mat_all_day.loc[dim_qx, dim_qy]:.2f}\\n\"\n",
    "            if kendall_pval_mat_all_day.loc[dim_qx, dim_qy] < 0.05:\n",
    "                annotations.loc[dim_qx, dim_qy] += f\"p={kendall_pval_mat_all_day.loc[dim_qx, dim_qy]:.3f}*\\n\"\n",
    "            else:\n",
    "                annotations.loc[dim_qx, dim_qy] += f\"p={kendall_pval_mat_all_day.loc[dim_qx, dim_qy]:.3f}\\n\"\n",
    "            annotations.loc[dim_qx, dim_qy] += f\"dl={int(kendall_len_mat_all_day.loc[dim_qx, dim_qy])}\\n\"\n",
    "            annotations.loc[dim_qx, dim_qy] += f\"norm={int(shapiro_pval_mat_all_day.loc[dim_qx, dim_qy])}\\n\"\n",
    "            annotations.loc[dim_qx, dim_qy] += f\"lag_dim={dim_qx}\"\n",
    "        else:\n",
    "            annotations.loc[dim_qx, dim_qy] = \"\"\n",
    "\n",
    "# Plot the combined heatmap with custom annotations (tau, p-value, and data length)\n",
    "plt.figure(figsize=(32, 32))\n",
    "sns.heatmap(kendall_mat_all_day, annot=annotations, fmt=\"\", cmap='coolwarm', square=True, cbar=True, mask=np.isnan(kendall_mat_all_day), annot_kws={\"size\": 12})\n",
    "title = 'Kendall Correlation, p-value, and Data Length for participant: '+ mainfolder.split(\"\\\\\")[-1]\n",
    "plt.title(title)\n",
    "plt.savefig(os.path.join(focused_folder, \"focused_all_day_subjective_objective_correlogram_fifteen_min_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#repeating annotations and figure for fdr correction\n",
    "annotations_fdr = kendall_mat_all_day.copy()\n",
    "for meas_1 in subOb_dict:\n",
    "    for meas_2 in subOb_dict:\n",
    "        if not np.isnan(kendall_mat_all_day.loc[meas_1, meas_2]):\n",
    "            annotations_fdr.loc[meas_1, meas_2] = f\"tau={kendall_mat_all_day.loc[meas_1, meas_2]:.2f}\\n\"\n",
    "            if kendall_pval_mat_all_day_fdr.loc[meas_1, meas_2] < 0.05:\n",
    "                annotations_fdr.loc[meas_1, meas_2] += f\"p={kendall_pval_mat_all_day_fdr.loc[meas_1, meas_2]:.2f}*\\n\"\n",
    "            else:\n",
    "                annotations_fdr.loc[meas_1, meas_2] += f\"p={kendall_pval_mat_all_day_fdr.loc[meas_1, meas_2]:.2f}\\n\"\n",
    "            annotations_fdr.loc[meas_1, meas_2] += f\"dl={int(kendall_len_mat_all_day.loc[meas_1, meas_2])}\\n\"\n",
    "            annotations_fdr.loc[meas_1, meas_2] += f\"norm={int(shapiro_pval_mat_all_day.loc[meas_1, meas_2])}\\n\"\n",
    "            annotations_fdr.loc[meas_1, meas_2] += f\"lag_dim={meas_1}\"\n",
    "        else:\n",
    "            annotations_fdr.loc[meas_1, meas_2] = \"\"\n",
    "\n",
    "# Plot the combined heatmap with custom annotations (tau, p-value, and data length)\n",
    "plt.figure(figsize=(32, 32))\n",
    "sns.heatmap(kendall_mat_all_day, annot=annotations_fdr, fmt=\"\", cmap='coolwarm', square=True, cbar=True, mask=np.isnan(kendall_mat_all_day), annot_kws={\"size\": 12})\n",
    "title = 'Kendall Correlation, p-value (fdr corrected), and Data Length for participant: '+ mainfolder.split(\"\\\\\")[-1]\n",
    "plt.title(title)\n",
    "plt.savefig(os.path.join(focused_folder, \"focused_all_day_subjective_objective_correlogram_fifteen_min_fdr_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#trying out dataclouds with FDR correction\n",
    "\n",
    "def create_datacloud_matrix(subOb_dict, subOb, mainfolder, kendall_mat_all_day, kendall_pval_mat_all_day, kendall_pval_mat_all_day_fdr, kendall_len_mat_all_day,lag, focused_folder, min_points=10):\n",
    "    \"\"\"\n",
    "    Create a matrix of scatter plots (dataclouds) corresponding to correlation analysis\n",
    "    Now uses FDR-corrected p-values for significance\n",
    "    \"\"\"\n",
    "    def lagged_per_day_vals(df, lag):\n",
    "        # Shift one column by lag (forward lag; negative for backward lag)\n",
    "        shifted = df.copy()\n",
    "        shifted.replace(-5000, np.nan, inplace=True)\n",
    "        col1 = df.columns[0]\n",
    "        col2 = df.columns[1]\n",
    "        shifted[col1] = shifted[col1].shift(lag)\n",
    "        # Drop rows with any NaN (either initial or from shift)\n",
    "        clean = shifted.dropna(subset=[col1, col2])\n",
    "        return clean\n",
    "\n",
    "    \n",
    "    special_var = {'eda': 30, 'pulse_rate':150, 'pulse_rate_variability':300, 'resp_rate':30, 'acc_std_dev':0.5}  #change AS REQUIRED \n",
    "    n_vars = len(subOb)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_vars, n_vars, figsize=(3*n_vars, 2*n_vars))\n",
    "    participant_title_list = mainfolder.split(\"\\\\\")[-1].split('_')[0:3]\n",
    "    participant_title = '_'.join(participant_title_list)\n",
    "    suptitle = 'Datacloud Matrix (FDR-corrected) for participant: '+ participant_title\n",
    "    fig.suptitle(suptitle, fontsize=26, y=0.98)\n",
    "    \n",
    "    # If only one variable, make axes 2D for consistent indexing\n",
    "    if n_vars == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_vars == 2:\n",
    "        axes = axes.reshape(2, 2)\n",
    "    \n",
    "    for i, dim_qx in enumerate(subOb):\n",
    "        for j, dim_qy in enumerate(subOb):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Collect data for this pair\n",
    "            dimqx = []\n",
    "            dimqy = []\n",
    "            \n",
    "            for subfolder in subOb_dict[dim_qx].keys():\n",
    "                if subfolder in subOb_dict[dim_qy].keys():\n",
    "                    temp_dimqx = []\n",
    "                    temp_dimqy = []\n",
    "                    for val_x in subOb_dict[dim_qx][subfolder].values():\n",
    "                        temp_dimqx.append(val_x)\n",
    "                    for val_y in subOb_dict[dim_qy][subfolder].values():\n",
    "                        temp_dimqy.append(val_y)\n",
    "                    temp_df = pd.DataFrame({f'{dim_qx}_1': temp_dimqx, f'{dim_qy}_2': temp_dimqy})\n",
    "                    clean_temp_df = lagged_per_day_vals(temp_df, lag)\n",
    "                    for val_x in clean_temp_df[f'{dim_qx}_1']:\n",
    "                        #print('appending shifted values of this day of the dimension: ', f'{dim_qx}')\n",
    "                        dimqx.append(val_x)\n",
    "                    for val_y in clean_temp_df[f'{dim_qy}_2']:\n",
    "                        #print('appending non-shifted values of this day of the dimension: ', f'{dim_qy}')\n",
    "                        dimqy.append(val_y)\n",
    "            \n",
    "            # Create DataFrame and filter data\n",
    "            filt_data = pd.DataFrame({f'{dim_qx}_1' : dimqx, f'{dim_qy}_2' : dimqy})\n",
    "            \n",
    "            \n",
    "            if len(filt_data) >= 2:\n",
    "                x_data = filt_data[f'{dim_qx}_1']\n",
    "                y_data = filt_data[f'{dim_qy}_2']\n",
    "                # Diagonal - do not leave blank this time for lagged correlations\n",
    "                # Create scatter plot\n",
    "                ax.scatter(x_data, y_data, alpha=0.6, s=20, color='steelblue')\n",
    "                if dim_qx in special_var.keys():\n",
    "                        for key, val in special_var.items():\n",
    "                            if dim_qx == key:\n",
    "                                ax.set_xlim(0, val)\n",
    "                else:\n",
    "                    ax.set_xlim(0, 4)\n",
    "                if dim_qy in special_var.keys():\n",
    "                        for key, val in special_var.items():\n",
    "                            if dim_qy == key:\n",
    "                                ax.set_ylim(0, val)\n",
    "                else:\n",
    "                    ax.set_ylim(0, 4)    \n",
    "\n",
    "                # Get correlation statistics from existing matrices\n",
    "                if len(filt_data) >= min_points and not np.isnan(kendall_mat_all_day.loc[dim_qx, dim_qy]):\n",
    "                        corr_ken = kendall_mat_all_day.loc[dim_qx, dim_qy]\n",
    "                        p_ken_raw = kendall_pval_mat_all_day.loc[dim_qx, dim_qy]\n",
    "                        p_ken_fdr = kendall_pval_mat_all_day_fdr.loc[dim_qx, dim_qy]\n",
    "                        n_points = int(kendall_len_mat_all_day.loc[dim_qx, dim_qy])\n",
    "                    \n",
    "                        \n",
    "                        # Add trend line if FDR-corrected correlation is significant and reasonably strong\n",
    "                        if not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05 : #and abs(corr_ken) > 0.2\n",
    "                            # Fit linear trend line\n",
    "                            z = np.polyfit(x_data, y_data, 1)\n",
    "                            p = np.poly1d(z)\n",
    "                            ax.plot(x_data.sort_values(), p(x_data.sort_values()), \n",
    "                                   \"r--\", alpha=0.8, linewidth=1.5)\n",
    "                        \n",
    "                        # Add correlation info with both raw and FDR p-values\n",
    "                        fdr_sig = \"*\" if (not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05) else \"\"\n",
    "                        raw_sig = \"*\" if p_ken_raw < 0.05 else \"\"\n",
    "                        \n",
    "                        annotation_text = f'tau={corr_ken:.3f}{fdr_sig}\\n'\n",
    "                        annotation_text += f'p_raw={p_ken_raw:.3f}{raw_sig}\\n'\n",
    "                        if not np.isnan(p_ken_fdr):\n",
    "                            annotation_text += f'p_fdr={p_ken_fdr:.3f}{fdr_sig}\\n'\n",
    "                        else:\n",
    "                            annotation_text += f'p_fdr=n/a\\n'\n",
    "                        annotation_text += f'n={n_points}'\n",
    "                        \n",
    "                        ax.text(0.05, 0.95, annotation_text,\n",
    "                               transform=ax.transAxes, fontsize=7,\n",
    "                               verticalalignment='top',\n",
    "                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                    \n",
    "                ax.set_xlabel(dim_qx+'_lag_'+str(lag), fontsize=9)\n",
    "                ax.set_ylabel(dim_qy, fontsize=9)\n",
    "                    \n",
    "                \n",
    "            else:\n",
    "                # Not enough data\n",
    "                ax.text(0.5, 0.5, f'Insufficient data\\n(n={len(filt_data)})', \n",
    "                       transform=ax.transAxes, ha='center', va='center',\n",
    "                       fontsize=10, color='red')\n",
    "                ax.set_xlabel(dim_qx, fontsize=9)\n",
    "                ax.set_ylabel(dim_qy, fontsize=9)\n",
    "                if dim_qx == special_var:\n",
    "                    ax.set_xlim(0, 30)\n",
    "                else:\n",
    "                    ax.set_xlim(0, 4)\n",
    "                if dim_qy == special_var:\n",
    "                    ax.set_ylim(0, 30)\n",
    "                else:\n",
    "                    ax.set_ylim(0, 4)\n",
    "            \n",
    "            # Clean up axes\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(focused_folder, \"focused_datacloud_matrix_fdr_fifteen_min_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_condensed_datacloud_matrix(subOb_dict, subOb, mainfolder, kendall_mat_all_day, kendall_pval_mat_all_day, kendall_pval_mat_all_day_fdr, kendall_len_mat_all_day, min_points=10):\n",
    "    \"\"\"\n",
    "    OPTIONALLY ADD SPECIFIC LIMITS LIKE IN THE PREVIOUS FUNCTION IF USING THIS FUNCTION\n",
    "    Create a condensed matrix showing only unique pairs (lower triangle)\n",
    "    Now uses FDR-corrected p-values for significance\n",
    "    \"\"\"\n",
    "    # Calculate number of unique pairs\n",
    "    n_vars = len(subOb)\n",
    "    n_unique_pairs = n_vars * (n_vars - 1) // 2\n",
    "    \n",
    "    if n_unique_pairs == 0:\n",
    "        print(\"Need at least 2 variables for correlation dataclouds\")\n",
    "        return\n",
    "    \n",
    "    # Determine subplot layout\n",
    "    cols = min(5, n_unique_pairs)  # Max 5 columns\n",
    "    rows = (n_unique_pairs + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    suptitle = 'Significant Correlations (FDR-corrected) - Dataclouds for participant: '+mainfolder.split(\"\\\\\")[-1]\n",
    "    fig.suptitle(suptitle, fontsize=14)\n",
    "    \n",
    "    # Make axes iterable\n",
    "    if n_unique_pairs == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if hasattr(axes, 'flatten') else axes\n",
    "    \n",
    "    plot_idx = 0\n",
    "    significant_pairs = []\n",
    "    \n",
    "    # Collect all pairs first (lower triangle)\n",
    "    for i, dim_qx in enumerate(subOb):\n",
    "        for j, dim_qy in enumerate(subOb):\n",
    "            if i <= j:  # Only lower triangle\n",
    "                continue\n",
    "                \n",
    "            # Collect and filter data\n",
    "            dimqx = []\n",
    "            dimqy = []\n",
    "            \n",
    "            for subfolder in subOb_dict[dim_qx].keys():\n",
    "                if subfolder in subOb_dict[dim_qy].keys():\n",
    "                    for val_x in subOb_dict[dim_qx][subfolder].values():\n",
    "                        dimqx.append(val_x)\n",
    "                    for val_y in subOb_dict[dim_qy][subfolder].values():\n",
    "                        dimqy.append(val_y)\n",
    "            \n",
    "            df = pd.DataFrame({f'{dim_qx}_1': dimqx, f'{dim_qy}_2': dimqy})\n",
    "            df.replace(-5000, np.nan, inplace=True)\n",
    "            filt_data = df.dropna()\n",
    "            \n",
    "            if len(filt_data) >= min_points and not np.isnan(kendall_mat_all_day.loc[dim_qx, dim_qy]):\n",
    "                corr_ken = kendall_mat_all_day.loc[dim_qx, dim_qy]\n",
    "                p_ken_raw = kendall_pval_mat_all_day.loc[dim_qx, dim_qy]\n",
    "                p_ken_fdr = kendall_pval_mat_all_day_fdr.loc[dim_qx, dim_qy]\n",
    "                n_points = int(kendall_len_mat_all_day.loc[dim_qx, dim_qy])\n",
    "                significant_pairs.append((dim_qx, dim_qy, filt_data, corr_ken, p_ken_raw, p_ken_fdr, n_points))\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    significant_pairs.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "    \n",
    "    # Plot significant pairs\n",
    "    for dim_qx, dim_qy, filt_data, corr_ken, p_ken_raw, p_ken_fdr, n_points in significant_pairs[:len(axes)]:\n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        x_data = filt_data[f'{dim_qx}_1']\n",
    "        y_data = filt_data[f'{dim_qy}_2']\n",
    "        \n",
    "        # Create scatter plot\n",
    "        ax.scatter(x_data, y_data, alpha=0.6, s=30, color='steelblue')\n",
    "        \n",
    "        # Add trend line for FDR-significant correlations\n",
    "        if not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05:\n",
    "            z = np.polyfit(x_data, y_data, 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_trend = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "            ax.plot(x_trend, p(x_trend), \"r-\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Add statistics with FDR information\n",
    "        fdr_significance = \"*\" if (not np.isnan(p_ken_fdr) and p_ken_fdr < 0.05) else \"\"\n",
    "        raw_significance = \"*\" if p_ken_raw < 0.05 else \"\"\n",
    "        \n",
    "        title_text = f'{dim_qx} vs {dim_qy}\\ntau={corr_ken:.3f}{fdr_significance}'\n",
    "        subtitle_text = f'p_raw={p_ken_raw:.3f}{raw_significance}'\n",
    "        if not np.isnan(p_ken_fdr):\n",
    "            subtitle_text += f', p_fdr={p_ken_fdr:.3f}{fdr_significance}'\n",
    "        subtitle_text += f', n={n_points}'\n",
    "        \n",
    "        ax.set_title(title_text, fontsize=10, fontweight='bold')\n",
    "        ax.text(0.5, -0.15, subtitle_text, transform=ax.transAxes, \n",
    "                ha='center', fontsize=8, style='italic')\n",
    "        \n",
    "        ax.set_xlabel(dim_qx, fontsize=9)\n",
    "        ax.set_ylabel(dim_qy, fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(plot_idx, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(mainfolder, \"filtered_significant_dataclouds_fdr_fifteen_min_timestitch.png\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Updated function calls with FDR correction:\n",
    "\n",
    "print(\"Creating full datacloud matrix with FDR correction...\")\n",
    "create_datacloud_matrix(subOb_dict, list(subOb_dict.keys()), mainfolder, \n",
    "                        kendall_mat_all_day, kendall_pval_mat_all_day, \n",
    "                        kendall_pval_mat_all_day_fdr, kendall_len_mat_all_day, lag, focused_folder)\n",
    "\n",
    "#print(\"Creating condensed datacloud matrix with FDR correction...\")\n",
    "#create_condensed_datacloud_matrix(subOb_dict, list(subOb_dict.keys()), mainfolder,\n",
    "                                 #kendall_mat_all_day, kendall_pval_mat_all_day,\n",
    "                                 #kendall_pval_mat_all_day_fdr, kendall_len_mat_all_day)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd12f78-34fd-4de3-a03b-ee38655def42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
